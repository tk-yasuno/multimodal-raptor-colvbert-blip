# GPU Test Guide
Visual RAPTOR ColBERT ã‚·ã‚¹ãƒ†ãƒ ã®GPUæœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Visual RAPTOR ColBERTã‚·ã‚¹ãƒ†ãƒ ã§ã®GPUä½¿ç”¨æ–¹æ³•ã€æ€§èƒ½ãƒ†ã‚¹ãƒˆã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ”§ GPUç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

- **GPU**: CUDAå¯¾å¿œGPUï¼ˆæ¨å¥¨ï¼šRTX 3060ä»¥ä¸Šã€8GB VRAMä»¥ä¸Šï¼‰
- **CUDA**: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 12.1ä»¥ä¸Š
- **Python**: 3.8-3.12
- **PyTorch**: CUDAç‰ˆï¼ˆ2.5.1+cu121ä»¥ä¸Šï¼‰

### 2. CUDA PyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# æ—¢å­˜ã®CPUç‰ˆPyTorchã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall torch torchvision torchaudio -y

# CUDAç‰ˆPyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 3. GPUèªè­˜ç¢ºèª

```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
print(f"GPU name: {torch.cuda.get_device_name(0)}")
```

## ğŸš€ GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆ

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ–¹æ³•

```bash
# GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
python test_gpu.py
```

### ãƒ†ã‚¹ãƒˆå†…å®¹

1. **GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯**
   - CUDAåˆ©ç”¨å¯èƒ½æ€§
   - GPUæƒ…å ±è¡¨ç¤º
   - ãƒ¡ãƒ¢ãƒªå®¹é‡ç¢ºèª

2. **ColBERT GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆ**
   - ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ€§èƒ½
   - ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ€§èƒ½  
   - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆæ€§èƒ½
   - GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–

3. **CPU vs GPUæ€§èƒ½æ¯”è¼ƒ**
   - å‡¦ç†é€Ÿåº¦æ¯”è¼ƒ
   - åŠ¹ç‡æ€§æ¸¬å®š

## ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆRTX 4060 Ti 16GBï¼‰

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠã‚ªãƒ—ã‚·ãƒ§ãƒ³

ã‚·ã‚¹ãƒ†ãƒ ã§ã¯2ã¤ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é¸æŠå¯èƒ½ï¼š

1. **ColModernVBERTEncoder** (SigLIPä½¿ç”¨) - æœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
2. **ColVBERTEncoder** (BLIPä½¿ç”¨) - å¾“æ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ColVBERTEncoderï¼ˆBLIPï¼‰æ€§èƒ½æŒ‡æ¨™

| é …ç›® | CPU | GPU | é€Ÿåº¦å‘ä¸Š |
|------|-----|-----|----------|
| ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† | ~20 texts/sec | 130.1 texts/sec | **6.5x** |
| ç”»åƒå‡¦ç† | ~2 images/sec | 31.4 images/sec | **15.7x** |
| ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | ~2 pairs/sec | 39.3 pairs/sec | **19.7x** |
| **ç·åˆé€Ÿåº¦å‘ä¸Š** | - | - | **17.7x** |

### ColModernVBERTEncoderï¼ˆSigLIPï¼‰æ€§èƒ½æŒ‡æ¨™

| é …ç›® | CPU | GPU | é€Ÿåº¦å‘ä¸Š |
|------|-----|-----|----------|
| ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† | ~7 texts/sec | 164.8 texts/sec | **23.5x** |
| ç”»åƒå‡¦ç† | ~2 images/sec | 52.6 images/sec | **26.3x** |
| ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | ~5 pairs/sec | 116.8 pairs/sec | **23.4x** |
| **ç·åˆé€Ÿåº¦å‘ä¸Š** | - | - | **23.2x** |

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼æ¯”è¼ƒ

| ç‰¹æ€§ | ColVBERT (BLIP) | ColModernVBERT (SigLIP) |
|------|-----------------|------------------------|
| **é€Ÿåº¦** | 17.7xå‘ä¸Š | 23.2xå‘ä¸Š â­ |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | 9.3% (1.48GB) | 2.6% (0.41GB) â­ |
| **å®‰å®šæ€§** | âœ… é«˜ã„ | âœ… é«˜ã„ |
| **ç²¾åº¦** | âœ… ç¢ºç«‹æ¸ˆã¿ | â­ æœ€æ–°æŠ€è¡“ |
| **äº’æ›æ€§** | âœ… å¹…åºƒã„ | âš ï¸ æ–°ã—ã„ |

### æ¨å¥¨ä½¿ç”¨ã‚±ãƒ¼ã‚¹

#### ColVBERTEncoderï¼ˆBLIPï¼‰ã‚’é¸æŠã™ã‚‹å ´åˆï¼š
- ğŸ¯ **å®‰å®šæ€§é‡è¦–**: ç¢ºç«‹ã•ã‚ŒãŸBLIPã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- ğŸ”§ **äº’æ›æ€§é‡è¦–**: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
- ğŸ“š **å®Ÿç¸¾é‡è¦–**: è±Šå¯Œãªåˆ©ç”¨å®Ÿç¸¾
- ğŸ’¾ **VRAMä½™è£•**: 16GBä»¥ä¸Šã®GPU

#### ColModernVBERTEncoderï¼ˆSigLIPï¼‰ã‚’é¸æŠã™ã‚‹å ´åˆï¼š
- âš¡ **æœ€é«˜æ€§èƒ½**: æœ€é€Ÿã®å‡¦ç†é€Ÿåº¦
- ğŸ’¾ **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: é™ã‚‰ã‚ŒãŸVRAMç’°å¢ƒ
- ğŸ”¬ **æœ€æ–°æŠ€è¡“**: æœ€å…ˆç«¯ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æŠ€è¡“
- ğŸš€ **å¤§è¦æ¨¡å‡¦ç†**: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿå‡¦ç†

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ

#### ColVBERTEncoderï¼ˆBLIPï¼‰
- **GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡**: 9.3% (1.48GB / 16GB)
- **åˆæœŸåŒ–ãƒ¡ãƒ¢ãƒª**: ~1.5GB VRAM
- **å‡¦ç†ä¸­ãƒ”ãƒ¼ã‚¯**: ~2-3GB VRAM
- **æ¨å¥¨VRAM**: 8GBä»¥ä¸Š

#### ColModernVBERTEncoderï¼ˆSigLIPï¼‰
- **GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡**: 2.6% (0.41GB / 16GB)
- **åˆæœŸåŒ–ãƒ¡ãƒ¢ãƒª**: ~0.4GB VRAM
- **å‡¦ç†ä¸­ãƒ”ãƒ¼ã‚¯**: ~1-2GB VRAM
- **æ¨å¥¨VRAM**: 4GBä»¥ä¸Š

### å®Ÿè¡Œé€Ÿåº¦è©³ç´°æ¯”è¼ƒ

#### ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†æ€§èƒ½
```
ColVBERT (BLIP):     130.1 texts/sec
ColModernVBERT:      164.8 texts/sec  (+26.7% faster)
```

#### ç”»åƒå‡¦ç†æ€§èƒ½
```
ColVBERT (BLIP):     31.4 images/sec
ColModernVBERT:      52.6 images/sec  (+67.5% faster)
```

#### ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†æ€§èƒ½
```
ColVBERT (BLIP):     39.3 pairs/sec
ColModernVBERT:      116.8 pairs/sec  (+197% faster)
```

## âš™ï¸ GPUæœ€é©åŒ–æ©Ÿèƒ½

### 1. è‡ªå‹•æœ€é©åŒ–

```python
# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠ
# Option 1: ColVBERT (BLIP) - å®‰å®šæ€§é‡è¦–
encoder = ColVBERTEncoder(
    device="auto"  # è‡ªå‹•ã§GPU/CPUé¸æŠ
)

# Option 2: ColModernVBERT (SigLIP) - æ€§èƒ½é‡è¦–
encoder = ColModernVBERTEncoder(
    device="auto"  # è‡ªå‹•ã§GPU/CPUé¸æŠ
)
```

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–ä¾‹

```python
# ColVBERTï¼ˆBLIPï¼‰- ãƒãƒ©ãƒ³ã‚¹å‹
colbert_encoder = ColVBERTEncoder(
    text_model_name="intfloat/multilingual-e5-large",
    vision_model_name="Salesforce/blip-image-captioning-base",
    embedding_dim=768,
    device="cuda"
)

# ColModernVBERTï¼ˆSigLIPï¼‰- é«˜æ€§èƒ½å‹
modern_encoder = ColModernVBERTEncoder(
    text_model_name="google/siglip-base-patch16-224",
    vision_model_name="google/siglip-base-patch16-224",
    embedding_dim=768,
    use_cross_attention=True,
    device="cuda"
)
```

### 2. ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

```python
# ColVBERTæ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º
text_embeddings = colbert_encoder.encode_text(
    texts, 
    batch_size=32  # ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼ˆBLIPã¯å°‘ã—æ§ãˆã‚ï¼‰
)

image_embeddings = colbert_encoder.encode_image(
    images, 
    batch_size=16  # ç”»åƒç”¨ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤§ï¼‰
)

# ColModernVBERTæ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º
text_embeddings = modern_encoder.encode_text(
    texts, 
    batch_size=64  # ãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼ˆSigLIPã¯åŠ¹ç‡çš„ï¼‰
)

image_embeddings = modern_encoder.encode_image(
    images, 
    batch_size=32  # ç”»åƒç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡è‰¯ã„ï¼‰
)
```

### 3. ãƒ¡ãƒ¢ãƒªç›£è¦–

```python
# GPUçŠ¶æ…‹ç¢ºèª
gpu_info = encoder.get_gpu_memory_info()
print(f"GPUä½¿ç”¨ç‡: {gpu_info['utilization_percent']:.1f}%")
print(f"ä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {gpu_info['allocated_gb']:.2f} GB")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
encoder.clear_gpu_cache()
```

## ğŸ”§ æœ€é©åŒ–è¨­å®š

### FP16æ··åˆç²¾åº¦

```python
# è‡ªå‹•ã§FP16ãŒæœ‰åŠ¹ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
encoder = ColModernVBERTEncoder()
# âœ… FP16æ··åˆç²¾åº¦ãŒæœ‰åŠ¹
# âœ… cuDNNæœ€é©åŒ–ãŒæœ‰åŠ¹
```

### ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´

| ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ | ãƒ‡ãƒ¼ã‚¿å‹ | æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º | VRAMä½¿ç”¨é‡ç›®å®‰ |
|-------------|---------|-----------------|---------------|
| **ColVBERT** | ãƒ†ã‚­ã‚¹ãƒˆ | 16-32 | ~2-3GB |
| **ColVBERT** | ç”»åƒ | 8-16 | ~4-6GB |
| **ColVBERT** | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | 8-16 | ~6-8GB |
| **ColModernVBERT** | ãƒ†ã‚­ã‚¹ãƒˆ | 32-64 | ~1-2GB |
| **ColModernVBERT** | ç”»åƒ | 16-32 | ~2-4GB |
| **ColModernVBERT** | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | 16-32 | ~4-6GB |

## ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. CUDA not available

**ç—‡çŠ¶**: `torch.cuda.is_available()` ãŒ `False`

**è§£æ±ºæ–¹æ³•**:
```bash
# CUDAç‰ˆPyTorchã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### 2. GPU Out of Memory

**ç—‡çŠ¶**: `CUDA out of memory` ã‚¨ãƒ©ãƒ¼

**è§£æ±ºæ–¹æ³•**:
```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
encoder.encode_text(texts, batch_size=8)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ32â†’8

# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
encoder.clear_gpu_cache()
```

#### 3. ä½ã„GPUä½¿ç”¨ç‡

**ç—‡çŠ¶**: GPUä½¿ç”¨ç‡ãŒ10%ä»¥ä¸‹

**è§£æ±ºæ–¹æ³•**:
```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—åŠ 
encoder.encode_text(texts, batch_size=64)  # 32â†’64

# ä¸¦åˆ—å‡¦ç†ã‚’ç¢ºèª
torch.backends.cudnn.benchmark = True
```

#### 4. å‡¦ç†ãŒé…ã„

**è§£æ±ºæ–¹æ³•**:
```python
# FP16ãŒæœ‰åŠ¹ã‹ç¢ºèª
print(f"Device: {encoder.device}")
print(f"FP16 enabled: {encoder.device == 'cuda'}")

# cuDNNæœ€é©åŒ–ç¢ºèª
print(f"cuDNN benchmark: {torch.backends.cudnn.benchmark}")
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

### GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª

```bash
# ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®GPUç›£è¦–
nvidia-smi

# é€£ç¶šç›£è¦–
nvidia-smi -l 1
```

### Pythonå†…ã§ã®ç›£è¦–

```python
import torch

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
allocated = torch.cuda.memory_allocated() / 1024**3
cached = torch.cuda.memory_reserved() / 1024**3
print(f"ä½¿ç”¨ä¸­: {allocated:.2f} GB")
print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cached:.2f} GB")

# GPUæ¸©åº¦ç¢ºèªï¼ˆnvidia-ml-pyãŒå¿…è¦ï¼‰
try:
    import pynvml
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
    print(f"GPUæ¸©åº¦: {temp}Â°C")
except:
    pass
```

## ğŸ¯ æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### GPUæœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠæŒ‡é‡

```python
# ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆï¼ˆ4-8GB VRAMï¼‰
encoder = ColModernVBERTEncoder()  # SigLIP - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å„ªå…ˆ

# å®‰å®šæ€§é‡è¦–ã®å ´åˆï¼ˆ8GB+ VRAMï¼‰
encoder = ColVBERTEncoder()  # BLIP - å®‰å®šæ€§å„ªå…ˆ

# æœ€é«˜æ€§èƒ½é‡è¦–ã®å ´åˆï¼ˆ16GB+ VRAMï¼‰
encoder = ColModernVBERTEncoder(  # SigLIP - æ€§èƒ½å„ªå…ˆ
    use_cross_attention=True
)
```

### 2. ãƒãƒƒãƒã‚µã‚¤ã‚ºè‡ªå‹•èª¿æ•´

```python
# GPU VRAMã«å¿œã˜ãŸè‡ªå‹•ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
def get_optimal_batch_size(encoder_type: str, data_type: str, vram_gb: float):
    if encoder_type == "ColVBERT":
        if vram_gb >= 16:
            return {"text": 32, "image": 16, "multimodal": 16}
        elif vram_gb >= 8:
            return {"text": 16, "image": 8, "multimodal": 8}
        else:
            return {"text": 8, "image": 4, "multimodal": 4}
    
    elif encoder_type == "ColModernVBERT":
        if vram_gb >= 16:
            return {"text": 64, "image": 32, "multimodal": 32}
        elif vram_gb >= 8:
            return {"text": 32, "image": 16, "multimodal": 16}
        else:
            return {"text": 16, "image": 8, "multimodal": 8}

# ä½¿ç”¨ä¾‹
vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
batch_sizes = get_optimal_batch_size("ColVBERT", "text", vram_gb)
```

### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆ¥ãƒ¡ãƒ¢ãƒªç®¡ç†

```python
# ColVBERTç”¨ãƒ¡ãƒ¢ãƒªç®¡ç†ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤§ï¼‰
def process_large_dataset_colbert(data, encoder):
    results = []
    
    for i, batch in enumerate(batches(data, batch_size=16)):  # æ§ãˆã‚ãƒãƒƒãƒã‚µã‚¤ã‚º
        result = encoder.encode_text(batch)
        results.append(result.cpu())  # CPUã«ç§»å‹•
        
        # é »ç¹ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆBLIPã¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤§ï¼‰
        if i % 5 == 0:
            encoder.clear_gpu_cache()
    
    return torch.cat(results)

# ColModernVBERTç”¨ãƒ¡ãƒ¢ãƒªç®¡ç†ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡è‰¯ã„ï¼‰
def process_large_dataset_modern(data, encoder):
    results = []
    
    for i, batch in enumerate(batches(data, batch_size=32)):  # å¤§ãã‚ãƒãƒƒãƒã‚µã‚¤ã‚º
        result = encoder.encode_text(batch)
        results.append(result.cpu())  # CPUã«ç§»å‹•
        
        # å°‘ãªã„é »åº¦ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        if i % 20 == 0:
            encoder.clear_gpu_cache()
    
    return torch.cat(results)
```

### 3. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆ¥ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
def safe_gpu_encode(encoder, data, encoder_type="ColVBERT"):
    """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆ¥ã®å®‰å…¨ãªGPUã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    try:
        if encoder_type == "ColVBERT":
            return encoder.encode_text(data, batch_size=16)
        else:  # ColModernVBERT
            return encoder.encode_text(data, batch_size=32)
            
    except torch.cuda.OutOfMemoryError:
        print("ğŸš¨ GPU Out of Memory - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ä¸­...")
        encoder.clear_gpu_cache()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆ¥ã® fallback ãƒãƒƒãƒã‚µã‚¤ã‚º
        fallback_batch = 8 if encoder_type == "ColVBERT" else 16
        batch_size = max(1, len(data) // 4, fallback_batch)
        
        return encoder.encode_text(data, batch_size=batch_size)
    
    except Exception as e:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        # CPU fallback
        encoder_cpu = type(encoder)(device="cpu")
        return encoder_cpu.encode_text(data)
```

## ğŸ“‹ GPUæ€§èƒ½ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### åŸºæœ¬è¦ä»¶
- [ ] CUDAå¯¾å¿œGPUæ­è¼‰
- [ ] CUDAç‰ˆPyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- [ ] `torch.cuda.is_available()` ãŒ `True`
- [ ] GPUæ¸©åº¦ãŒ85Â°Cä»¥ä¸‹
- [ ] ååˆ†ãªVRAMå®¹é‡ï¼ˆæ¨å¥¨8GBä»¥ä¸Šï¼‰

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆ¥ãƒã‚§ãƒƒã‚¯
#### ColVBERTEncoderï¼ˆBLIPï¼‰
- [ ] 8GBä»¥ä¸Šã®VRAM
- [ ] ãƒãƒƒãƒã‚µã‚¤ã‚º: ãƒ†ã‚­ã‚¹ãƒˆ16-32, ç”»åƒ8-16
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡9-10%ä»¥ä¸‹ï¼ˆ16GB GPUï¼‰
- [ ] å‡¦ç†é€Ÿåº¦: 100+ texts/sec, 25+ images/sec

#### ColModernVBERTEncoderï¼ˆSigLIPï¼‰
- [ ] 4GBä»¥ä¸Šã®VRAM
- [ ] ãƒãƒƒãƒã‚µã‚¤ã‚º: ãƒ†ã‚­ã‚¹ãƒˆ32-64, ç”»åƒ16-32
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡3-5%ä»¥ä¸‹ï¼ˆ16GB GPUï¼‰
- [ ] å‡¦ç†é€Ÿåº¦: 150+ texts/sec, 45+ images/sec

### æœ€é©åŒ–æ©Ÿèƒ½
- [ ] FP16æ··åˆç²¾åº¦ãŒæœ‰åŠ¹
- [ ] cuDNNæœ€é©åŒ–ãŒæœ‰åŠ¹
- [ ] é©åˆ‡ãªãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
- [ ] ãƒ¡ãƒ¢ãƒªç›£è¦–æ©Ÿèƒ½å‹•ä½œ
- [ ] è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢æ©Ÿèƒ½

## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯

- [PyTorch CUDA Installation](https://pytorch.org/get-started/locally/)
- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
- [cuDNN Installation Guide](https://docs.nvidia.com/deeplearning/cudnn/install-guide/)

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å«ã‚ã¦ãŠå•ã„åˆã‚ã›ãã ã•ã„ï¼š

### åŸºæœ¬æƒ…å ±
1. `nvidia-smi` ã®å‡ºåŠ›
2. `torch.cuda.is_available()` ã®çµæœ
3. PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³: `torch.__version__`
4. ä½¿ç”¨ã—ã¦ã„ã‚‹GPUãƒ¢ãƒ‡ãƒ«ã¨VRAMå®¹é‡

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼æƒ…å ±
5. ä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: `ColVBERTEncoder` ã¾ãŸã¯ `ColModernVBERTEncoder`
6. åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ/ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«åï¼‰
7. è¨­å®šã—ãŸãƒãƒƒãƒã‚µã‚¤ã‚º

### ã‚¨ãƒ©ãƒ¼æƒ…å ±
8. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¨æ–‡
9. GPUä½¿ç”¨ç‡: `encoder.get_gpu_memory_info()`
10. å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆæ•°ã€ç”»åƒæ•°ï¼‰

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
11. å®Ÿæ¸¬å‡¦ç†é€Ÿåº¦ï¼ˆtexts/sec, images/secï¼‰
12. æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½ã¨ã®å·®
13. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤‰åŒ–

---

**æœ€çµ‚æ›´æ–°**: 2025å¹´10æœˆ25æ—¥  
**ãƒ†ã‚¹ãƒˆç’°å¢ƒ**: 
- GPU: NVIDIA GeForce RTX 4060 Ti (16GB VRAM)
- ColVBERTEncoder: 17.7xé€Ÿåº¦å‘ä¸Š, 9.3% VRAMä½¿ç”¨
- ColModernVBERTEncoder: 23.2xé€Ÿåº¦å‘ä¸Š, 2.6% VRAMä½¿ç”¨