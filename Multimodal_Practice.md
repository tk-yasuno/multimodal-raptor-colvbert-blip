# Multimodal RAPTOR: 46PDFå¤§è¦æ¨¡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè£…è¨˜éŒ²

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

### ç›®æ¨™
- **ã‚¹ãƒ¢ãƒ¼ãƒ«ã‚¹ã‚¿ãƒ‡ã‚£**: 2 PDFs (131ãƒšãƒ¼ã‚¸) â†’ **æœ¬ç•ªã‚¹ã‚±ãƒ¼ãƒ«**: 46 PDFs (2378ãƒšãƒ¼ã‚¸)
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿(ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆ)ã‚’æ´»ç”¨ã—ãŸRAPTOR Treeæ§‹ç¯‰
- GPUä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- å®Ÿç”¨çš„ãªå‡¦ç†æ™‚é–“ã®å®Ÿç¾

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **PDFæ–‡æ›¸æ•°**: 46æ–‡æ›¸
- **ç·ãƒšãƒ¼ã‚¸æ•°**: 2378ãƒšãƒ¼ã‚¸
- **ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«**: 2378æš (150 DPI PNG)
- **ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯**: 4250å€‹ (RecursiveCharacterTextSplitter: 800 tokens, 150 overlap)
- **å¹³å‡ãƒãƒ£ãƒ³ã‚¯æ•°/ãƒšãƒ¼ã‚¸**: 1.8

---

## ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ•ãƒ­ãƒ¼

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦ï¼ˆMermaidå›³ï¼‰

```mermaid
flowchart TB
    subgraph VD["Visual Document"]
        IMG["ğŸ“· image_path<br/>(PNG file)"]
        TXT["ğŸ“„ page_content<br/>(OCR extracted text)"]
    end
    
    subgraph ImgPath["Image Processing Pipeline"]
        IE["ğŸ¨ Image Encoder<br/>(Vision Model)"]
        BLIP["ColVBERT: BLIP Vision<br/>Output: 768-dim"]
        SIGLIP["ColModernVBERT: SigLIP<br/>Output: 768-dim"]
        FP16["âš¡ FP16 Mixed Precision<br/>torch.amp.autocast"]
        VP["ğŸ”„ Vision Projection<br/>768 â†’ 1024 dim<br/>+ L2 Normalize"]
    end
    
    subgraph TextPath["Text Processing Pipeline"]
        TE["ğŸ“ Text Encoder<br/>(Language Model)"]
        HF["HuggingFaceEmbeddings<br/>mxbai-embed-large-v1<br/>Output: 1024-dim"]
        GPU["âš¡ GPU Batch Processing<br/>batch_size=64<br/>L2 Normalized"]
    end
    
    subgraph Fusion["Multimodal Fusion"]
        WA["âš–ï¸ Weighted Average<br/>w = 0.3 (image_weight)<br/><br/>embedding =<br/>(1-w) Ã— text_emb<br/>+ w Ã— image_emb<br/><br/>= 0.7 Ã— text + 0.3 Ã— image"]
        NORM["ğŸ”„ L2 Normalize<br/>(1024-dim vector)"]
    end
    
    subgraph RAPTOR["RAPTOR Tree Construction"]
        TREE["ğŸŒ³ Hierarchical Tree<br/>Silhouette Clustering<br/>max_depth = 3"]
        D0["Depth 0: 4250 leaf nodes"]
        D1["Depth 1: ~850 parent nodes"]
        D2["Depth 2: ~170 parent nodes"]
        D3["Depth 3: ~35 root nodes"]
    end
    
    IMG --> IE
    IE --> BLIP & SIGLIP
    BLIP & SIGLIP --> FP16
    FP16 --> VP
    
    TXT --> TE
    TE --> HF
    HF --> GPU
    
    VP --> WA
    GPU --> WA
    WA --> NORM
    NORM --> TREE
    TREE --> D0
    D0 --> D1
    D1 --> D2
    D2 --> D3
    
    style VD fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    style ImgPath fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style TextPath fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style Fusion fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    style RAPTOR fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    style IMG fill:#bbdefb,stroke:#1976d2
    style TXT fill:#bbdefb,stroke:#1976d2
    style FP16 fill:#ffcc80,stroke:#ef6c00
    style GPU fill:#ce93d8,stroke:#6a1b9a
    style WA fill:#a5d6a7,stroke:#2e7d32
    style TREE fill:#f48fb1,stroke:#c2185b
```

### è©³ç´°å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆMermaid ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³ï¼‰

```mermaid
sequenceDiagram
    participant VD as Visual Document
    participant IMG as Image Pipeline
    participant TXT as Text Pipeline
    participant FUSE as Fusion Layer
    participant TREE as RAPTOR Tree
    
    Note over VD: 4250 Documents<br/>107 Batches (size=40)
    
    rect rgb(255, 243, 224)
    Note over IMG: Image Encoding
    VD->>IMG: image_path (PNG)
    IMG->>IMG: Load PIL Image<br/>Convert to RGB
    IMG->>IMG: Vision Processor<br/>(Resize, Normalize)
    IMG->>IMG: FP16 Autocast ON
    IMG->>IMG: BLIP/SigLIP Forward<br/>(768-dim)
    IMG->>IMG: Vision Projection<br/>(768â†’1024)
    IMG->>IMG: L2 Normalize
    IMG->>IMG: FP16 â†’ FP32
    Note over IMG: ~2 sec/batch
    end
    
    rect rgb(243, 229, 245)
    Note over TXT: Text Encoding
    VD->>TXT: page_content (text)
    TXT->>TXT: Tokenization<br/>(max_length=512)
    TXT->>TXT: GPU Batch Forward<br/>(batch_size=64)
    TXT->>TXT: Mean Pooling
    TXT->>TXT: L2 Normalize
    TXT->>TXT: Output (1024-dim)
    Note over TXT: ~0.5 sec/batch
    end
    
    rect rgb(232, 245, 233)
    Note over FUSE: Multimodal Fusion
    IMG->>FUSE: image_emb (1024)
    TXT->>FUSE: text_emb (1024)
    FUSE->>FUSE: GPU Tensor Convert
    FUSE->>FUSE: Weighted Average<br/>0.7Ã—text + 0.3Ã—image
    FUSE->>FUSE: L2 Normalize
    FUSE->>FUSE: CPU/Numpy Convert
    Note over FUSE: ~0.1 sec/batch
    end
    
    rect rgb(252, 228, 236)
    Note over TREE: Tree Construction
    FUSE->>TREE: Multimodal Embeddings<br/>(4250, 1024)
    TREE->>TREE: Depth 0: Leaf Nodes<br/>(4250 docs)
    TREE->>TREE: Silhouette Clustering<br/>(k=2-5)
    TREE->>TREE: Depth 1: Parent Nodes<br/>(~850 summaries)
    TREE->>TREE: Silhouette Clustering
    TREE->>TREE: Depth 2: Parent Nodes<br/>(~170 summaries)
    TREE->>TREE: Silhouette Clustering
    TREE->>TREE: Depth 3: Root Nodes<br/>(~35 summaries)
    Note over TREE: Total: ~2 hours
    end
    
    TREE-->>VD: âœ… Complete
```

### ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ï¼ˆMermaidå›³ï¼‰

```mermaid
flowchart TD
    A["4250 Visual Documents"]
    
    B["Batch 1: 40 docs"]
    C["Batch 2: 40 docs"]
    D["Batch 3: 40 docs"]
    E["... 104 more batches"]
    F["Batch 107: 10 docs"]
    
    B1["40 Images BLIP/SigLIP"]
    B2["40 Texts HuggingFace"]
    B3["Fusion 0.7Ã—text + 0.3Ã—img"]
    B4["Output 40x1024"]
    
    G["vstack all batches"]
    H["Embedding Matrix 4250x1024"]
    I["Hierarchical Clustering"]
    J["RAPTOR Tree max_depth=3"]
    
    A --> B
    A --> C
    A --> D
    A --> E
    A --> F
    
    B --> B1
    B --> B2
    B1 --> B3
    B2 --> B3
    B3 --> B4
    
    C --> B4
    D --> B4
    E --> B4
    F --> B4
    
    B4 --> G
    G --> H
    H --> I
    I --> J
    
    style A fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    style B fill:#fff3e0,stroke:#e65100
    style C fill:#fff3e0,stroke:#e65100
    style D fill:#fff3e0,stroke:#e65100
    style E fill:#fff3e0,stroke:#e65100
    style F fill:#fff3e0,stroke:#e65100
    style B1 fill:#f3e5f5,stroke:#4a148c
    style B2 fill:#f3e5f5,stroke:#4a148c
    style B3 fill:#f3e5f5,stroke:#4a148c
    style B4 fill:#f3e5f5,stroke:#4a148c
    style G fill:#e8f5e9,stroke:#1b5e20
    style H fill:#e8f5e9,stroke:#1b5e20
    style I fill:#fce4ec,stroke:#880e4f
    style J fill:#fce4ec,stroke:#880e4f
```

### è©³ç´°ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—

#### Step 1: Visual Document ã®æº–å‚™
```python
class VisualDocument(Document):
    image_path: str          # PNGç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    page_content: str        # OCRæŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    metadata: dict          # ãƒšãƒ¼ã‚¸ç•ªå·ã€PDFåãªã©
```

#### Step 2: ç”»åƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ (ColVBERT/ColModernVBERT)

**ColVBERT (BLIP) ã®å ´åˆ:**
```python
def encode_image(self, images: List[Image.Image]) -> torch.Tensor:
    """ç”»åƒ â†’ 1024æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«"""
    
    # 1. ç”»åƒå‰å‡¦ç†
    inputs = self.vision_processor(
        images=images,
        return_tensors="pt"
    ).to(self.device)  # GPUè»¢é€
    
    # 2. FP16æ··åˆç²¾åº¦ã§æ¨è«–
    with torch.no_grad():
        with torch.amp.autocast('cuda', enabled=(self.device == "cuda")):
            # BLIP Vision Model (768-dim)
            outputs = self.vision_encoder.vision_model(inputs['pixel_values'])
            image_features = outputs.last_hidden_state.mean(dim=1)  # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
            
            # 3. æŠ•å½±å±¤ã§1024æ¬¡å…ƒã«å¤‰æ›
            image_features = self.vision_projection(image_features)  # 768â†’1024
            
            # 4. L2æ­£è¦åŒ–
            image_features = nn.functional.normalize(image_features, p=2, dim=1)
    
    return image_features.float()  # FP32ã«æˆ»ã™
```

**ColModernVBERT (SigLIP) ã®å ´åˆ:**
```python
def encode_image(self, images: List[Image.Image]) -> torch.Tensor:
    """ç”»åƒ â†’ 1024æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«"""
    
    inputs = self.processor(
        images=images,
        return_tensors="pt"
    ).to(self.device)
    
    with torch.no_grad():
        with torch.amp.autocast('cuda', enabled=(self.device == "cuda")):
            # SigLIPçµ±åˆãƒ¢ãƒ‡ãƒ« (768-dim)
            outputs = self.model.get_image_features(**inputs)
            
            # æŠ•å½±å±¤ã§1024æ¬¡å…ƒã«
            embeddings = self.projection(outputs)
            
            # L2æ­£è¦åŒ–
            embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
    
    return embeddings.float()
```

#### Step 3: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ (HuggingFace)

```python
# HuggingFaceEmbeddingsè¨­å®š
text_embeddings = HuggingFaceEmbeddings(
    model_name="mixedbread-ai/mxbai-embed-large-v1",
    model_kwargs={'device': 'cuda'},
    encode_kwargs={
        'normalize_embeddings': True,  # L2æ­£è¦åŒ–ã‚’è‡ªå‹•é©ç”¨
        'batch_size': 64               # GPUä¸¦åˆ—å‡¦ç†
    }
)

# ãƒ†ã‚­ã‚¹ãƒˆ â†’ 1024æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« (æ—¢ã«æ­£è¦åŒ–æ¸ˆã¿)
text_emb = text_embeddings.embed_documents([doc.page_content])
# å‡ºåŠ›: numpy.ndarray, shape=(1, 1024)
```

#### Step 4: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆ

```python
def embed_documents(self, documents: List[Document], batch_size: int = 40):
    """Visual Documentã‚’ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›"""
    
    all_embeddings = []
    
    for batch in batches(documents, batch_size):
        # 1. ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        texts = [doc.page_content for doc in batch]
        text_embeddings = self.text_embeddings.embed_documents(texts)
        # shape: (batch_size, 1024), dtype: float32
        
        # 2. ç”»åƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        images = [Image.open(doc.image_path).convert('RGB') for doc in batch]
        image_embeddings = self.visual_encoder.encode_image(images)
        # shape: (batch_size, 1024), dtype: float32
        
        # 3. GPUä¸Šã§èåˆ
        text_emb_tensor = torch.tensor(text_embeddings, device=self.device)
        image_emb_tensor = image_embeddings.to(self.device)
        
        # é‡ã¿ä»˜ãå¹³å‡ (w=0.3)
        multimodal_emb = (1 - self.multimodal_weight) * text_emb_tensor + \
                         self.multimodal_weight * image_emb_tensor
        
        # å†æ­£è¦åŒ–
        multimodal_emb = nn.functional.normalize(multimodal_emb, p=2, dim=1)
        
        # 4. CPU/Numpyã«å¤‰æ›
        all_embeddings.append(multimodal_emb.cpu().numpy())
    
    # 5. å…¨ãƒãƒƒãƒã‚’çµåˆ
    final_embeddings = np.vstack(all_embeddings)
    # shape: (total_docs, 1024)
    
    return final_embeddings
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§

| å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— | å…¥åŠ›ã‚µã‚¤ã‚º | å‡ºåŠ›ã‚µã‚¤ã‚º | å‡¦ç†æ™‚é–“ (æ¨å®š) | GPUä½¿ç”¨ |
|------------|---------|-----------|--------------|--------|
| ç”»åƒèª­ã¿è¾¼ã¿ | 40 images | 40 PIL objects | ~1ç§’ | âŒ |
| ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ | (40, 3, 224, 224) | (40, 1024) | ~2ç§’ | âœ… FP16 |
| ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ | 40 texts | (40, 1024) | ~0.5ç§’ | âœ… batch=64 |
| ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆ | 2Ã—(40, 1024) | (40, 1024) | ~0.1ç§’ | âœ… |
| **ãƒãƒƒãƒåˆè¨ˆ** | 40 docs | (40, 1024) | **~3.6ç§’** | - |
| **å…¨ä½“ (107 batches)** | 4250 docs | (4250, 1024) | **~385ç§’ (6.4åˆ†)** | - |

### é‡è¦ãªè¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ

1. **æ¬¡å…ƒçµ±ä¸€**: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜1024æ¬¡å…ƒã«æŠ•å½±
   - æŠ•å½±å‰: ç”»åƒ=768, ãƒ†ã‚­ã‚¹ãƒˆ=1024
   - æŠ•å½±å¾Œ: ä¸¡æ–¹=1024
   - ç†ç”±: å˜ç´”ãªé‡ã¿ä»˜ãå¹³å‡ã‚’å¯èƒ½ã«ã™ã‚‹

2. **L2æ­£è¦åŒ–**: 3ç®‡æ‰€ã§é©ç”¨
   - ç”»åƒåŸ‹ã‚è¾¼ã¿å¾Œ
   - ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿å¾Œ
   - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆå¾Œ
   - ç†ç”±: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ç²¾åº¦å‘ä¸Š

3. **FP16æ··åˆç²¾åº¦**: ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ã¿
   - é©ç”¨: BLIP/SigLIP vision models
   - éé©ç”¨: HuggingFace text embeddings
   - ç†ç”±: ç”»åƒå‡¦ç†ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¯ååˆ†é«˜é€Ÿ

4. **ãƒãƒƒãƒã‚µã‚¤ã‚ºã®åˆ†é›¢**
   - ç”»åƒãƒãƒƒãƒ: 40 (ãƒ¡ãƒ¢ãƒªåˆ¶ç´„)
   - ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ: 64 (é«˜é€ŸåŒ–é‡è¦–)
   - ç†ç”±: ç”»åƒã¯ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãŒå¤§ãã„

---

## å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚ºã¨æ•™è¨“

### Phase 1: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¤±æ•—ã¨ãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ–

#### å•é¡Œ
```
åˆæœŸè©¦è¡Œ: 2378ãƒšãƒ¼ã‚¸ â†’ 5927ãƒãƒ£ãƒ³ã‚¯ (500 tokens, 100 overlap)
çµæœ: Ollama context limitè¶…éã§ã‚¨ãƒ©ãƒ¼
```

#### è§£æ±ºç­–
```python
# ãƒãƒ£ãƒ³ã‚¯è¨­å®šã®æœ€é©åŒ–
RecursiveCharacterTextSplitter(
    chunk_size=800,        # 500 â†’ 800 (60%å¢—åŠ )
    chunk_overlap=150,     # 100 â†’ 150 (50%å¢—åŠ )
    length_function=len
)

çµæœ: 5927 â†’ 4250ãƒãƒ£ãƒ³ã‚¯ (28%å‰Šæ¸›)
```

#### æ•™è¨“
âœ… **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯æ–‡è„ˆä¿æŒã¨ãƒãƒ£ãƒ³ã‚¯æ•°ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦**
- ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºå¢—åŠ ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒ
- ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å¢—åŠ ã§å¢ƒç•Œæƒ…å ±ã®æ¬ è½ã‚’é˜²æ­¢
- ç·ãƒãƒ£ãƒ³ã‚¯æ•°å‰Šæ¸›ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š

---

### Phase 2: æœªä½¿ç”¨ã®ç”»åƒåŸ‹ã‚è¾¼ã¿ç™ºè¦‹

#### å•é¡Œ
```python
# visual_raptor_colbert.py ã®å®Ÿè£…
class VisualRAPTORColBERT(TsunamiLessonRAPTOR):
    def __init__(self, ...):
        super().__init__(
            domain=domain,
            cluster_strategy=cluster_strategy,
            # âŒ visual_encoderã‚’æ¸¡ã—ã¦ã„ãªã„
            # âŒ use_multimodal=False (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        )
```

**çµæœ**: ç”»åƒåŸ‹ã‚è¾¼ã¿ã¯ç”Ÿæˆã•ã‚ŒãŸãŒã€RAPTOR Treeã®æ§‹ç¯‰ã«ã¯ä½¿ã‚ã‚Œã¦ã„ãªã‹ã£ãŸ

#### è§£æ±ºç­–
```python
class VisualRAPTORColBERT(TsunamiLessonRAPTOR):
    def __init__(self, ...):
        super().__init__(
            domain=domain,
            cluster_strategy=cluster_strategy,
            visual_encoder=self.colbert_encoder,  # âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’æ¸¡ã™
            use_multimodal=True,                   # âœ… ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æœ‰åŠ¹åŒ–
            multimodal_weight=multimodal_weight
        )
```

#### æ•™è¨“
âœ… **ç¶™æ‰¿ã‚¯ãƒ©ã‚¹ã§ã¯è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»•æ§˜ã‚’æ­£ç¢ºã«æŠŠæ¡ã™ã‚‹**
- visual_encoderã¨use_multimodalã®ä¸¡æ–¹ãŒå¿…è¦
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«ä¾å­˜ã›ãšæ˜ç¤ºçš„ã«è¨­å®š
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èåˆã®é‡ã¿(30% image, 70% text)ã‚‚èª¿æ•´å¯èƒ½

---

### Phase 3: Ollama HTTPåŸ‹ã‚è¾¼ã¿ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

#### å•é¡Œ
```python
# å¾“æ¥ã®å®Ÿè£…
text_embeddings = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

æ¨å®šå‡¦ç†æ™‚é–“: 40åˆ†ä»¥ä¸Š (4250ãƒãƒ£ãƒ³ã‚¯)
å•é¡Œ: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã€ã‚·ãƒªã‚¢ãƒ«å‡¦ç†
```

#### è§£æ±ºç­–
```python
# GPUåŠ é€Ÿç‰ˆ
from langchain_huggingface import HuggingFaceEmbeddings

text_embeddings = HuggingFaceEmbeddings(
    model_name="mixedbread-ai/mxbai-embed-large-v1",
    model_kwargs={'device': 'cuda'},
    encode_kwargs={
        'normalize_embeddings': True,
        'batch_size': 64  # ãƒãƒƒãƒå‡¦ç†
    }
)

å®Ÿæ¸¬å‡¦ç†æ™‚é–“: ç´„7åˆ† (4250ãƒãƒ£ãƒ³ã‚¯, depth 0)
é«˜é€ŸåŒ–: 10-15å€
```

#### æ•™è¨“
âœ… **ãƒ­ãƒ¼ã‚«ãƒ«GPUæ´»ç”¨ã§HTTPã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ’é™¤**
- sentence-transformersã«ã‚ˆã‚‹ç›´æ¥GPUå®Ÿè¡Œ
- ãƒãƒƒãƒå‡¦ç†ã§ä¸¦åˆ—åŒ–
- normalize_embeddings=Trueã§æ¤œç´¢ç²¾åº¦å‘ä¸Š

---

### Phase 4: query()ãƒ¡ã‚½ãƒƒãƒ‰ä¸å­˜åœ¨ã‚¨ãƒ©ãƒ¼

#### å•é¡Œ
```python
# è©•ä¾¡ã‚³ãƒ¼ãƒ‰
results = colbert_system.query(query, k=5)
# AttributeError: 'VisualRAPTORColBERT' object has no attribute 'query'
```

#### è§£æ±ºç­–
```python
# æ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰å
results = colbert_system.retrieve(query, k=5)
```

#### æ•™è¨“
âœ… **åŸºåº•ã‚¯ãƒ©ã‚¹ã®APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèª**
- RAPTORã‚¯ãƒ©ã‚¹ã¯retrieve()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æä¾›
- query()ã¯åˆ¥ã®ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰å
- IDEã®è‡ªå‹•è£œå®Œã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§ã‚’æ´»ç”¨

---

### Phase 5: Treeå†æ§‹ç¯‰ã®æ™‚é–“æå¤±

#### å•é¡Œ
```
RAPTOR Treeæ§‹ç¯‰æ™‚é–“: 178.9åˆ† (ç´„3æ™‚é–“)
å•é¡Œ: è©•ä¾¡ã®ãŸã³ã«æ¯å›å†æ§‹ç¯‰ãŒå¿…è¦
```

#### è§£æ±ºç­–
```python
import pickle
from pathlib import Path

# Treeä¿å­˜
colbert_tree_pickle = trees_dir / "colbert_blip_tree_46pdfs_chunked.pkl"

# æ—¢å­˜Treeã®èª­ã¿è¾¼ã¿
if colbert_tree_pickle.exists():
    print(f"ğŸ“ æ—¢å­˜ã®Treeã‚’èª­ã¿è¾¼ã¿ä¸­: {colbert_tree_pickle.name}")
    with open(colbert_tree_pickle, 'rb') as f:
        tree_data = pickle.load(f)
        colbert_tree = tree_data['tree']
        colbert_tree_build_time = tree_data['build_time']
        colbert_tree_stats = tree_data.get('stats', {})
else:
    # æ–°è¦æ§‹ç¯‰
    print("ğŸŒ³ æ–°è¦ã«RAPTOR Treeã‚’æ§‹ç¯‰ä¸­...")
    colbert_tree = colbert_system.build_tree(visual_documents)
    
    # ä¿å­˜
    with open(colbert_tree_pickle, 'wb') as f:
        pickle.dump({
            'tree': colbert_tree,
            'build_time': colbert_tree_build_time,
            'stats': colbert_tree_stats
        }, f)
    print(f"ğŸ’¾ Treeã‚’ä¿å­˜: {colbert_tree_pickle}")

å†æ§‹ç¯‰æ™‚é–“: 0ç§’ (å³åº§ã«ãƒ­ãƒ¼ãƒ‰)
```

#### æ•™è¨“
âœ… **é«˜ã‚³ã‚¹ãƒˆè¨ˆç®—ã®çµæœã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹**
- pickleã«ã‚ˆã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(build_time, stats)ã‚‚ä¸€ç·’ã«ä¿å­˜
- ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã§è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ

---

### Phase 6: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºï¼‰

#### å®Ÿé¨“
```python
# åˆæœŸè¨­å®š
def embed_documents(self, documents, batch_size: int = 40):
    # 4250ãƒãƒ£ãƒ³ã‚¯ Ã· 40 = 107ãƒãƒƒãƒ

# æœ€é©åŒ–1
batch_size = 80
# 4250ãƒãƒ£ãƒ³ã‚¯ Ã· 80 = 54ãƒãƒƒãƒ (50%å‰Šæ¸›)

# æœ€é©åŒ–2 (å®‰å®šæ€§é‡è¦–)
batch_size = 50
# 4250ãƒãƒ£ãƒ³ã‚¯ Ã· 50 = 85ãƒãƒƒãƒ

# æœ€çµ‚è¨­å®š (ãƒ¡ãƒ¢ãƒªå®‰å®šæ€§)
batch_size = 40
# 4250ãƒãƒ£ãƒ³ã‚¯ Ã· 40 = 107ãƒãƒƒãƒ
```

#### ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã®ãƒãƒƒãƒã‚µã‚¤ã‚º
```python
HuggingFaceEmbeddings(
    encode_kwargs={
        'batch_size': 32  # åˆæœŸ
        'batch_size': 64  # æœ€é©åŒ– (2å€)
    }
)
```

#### æ•™è¨“
âœ… **ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯GPUãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**
- RTX 4060 Ti (16GB VRAM)ã§ã¯40-50ãŒå®‰å®š
- ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã¯64ã§ã‚‚å•é¡Œãªã—
- ç”»åƒå‡¦ç†ã¯å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ¡ãƒ¢ãƒªåœ§è¿«

---

### Phase 7: FP16æ··åˆç²¾åº¦ã®å°å…¥

#### å®Ÿè£…
```python
class ColVBERTEncoder:
    def __init__(self, device="cuda"):
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        self.vision_encoder = BlipForConditionalGeneration.from_pretrained(...)
        self.text_encoder = AutoModel.from_pretrained(...)
        
        # FP16æ··åˆç²¾åº¦ã®æœ‰åŠ¹åŒ–
        if self.device == "cuda":
            self.vision_encoder = self.vision_encoder.half()
            self.text_projection = self.text_projection.half()
            self.vision_projection = self.vision_projection.half()
            self.fusion_layer = self.fusion_layer.half()
    
    def encode_image(self, images):
        with torch.no_grad():
            with torch.amp.autocast('cuda', enabled=(self.device == "cuda")):
                outputs = self.vision_encoder.vision_model(inputs['pixel_values'])
                image_features = self.vision_projection(image_features)
                image_features = nn.functional.normalize(image_features, p=2, dim=1)
        
        return image_features.float()  # FP32ã«æˆ»ã™
```

#### ColModernVBERT (SigLIP)ã§ã‚‚åŒæ§˜
```python
class ColModernVBERTEncoder:
    def __init__(self, device="cuda"):
        if self.device == "cuda":
            self.model = self.model.half()
            self.projection = self.projection.half()
            self.fusion_layer = self.fusion_layer.half()
        print(f"   FP16: {self.device == 'cuda'}")
    
    def encode_text(self, texts):
        with torch.no_grad():
            with torch.amp.autocast('cuda', enabled=(self.device == "cuda")):
                outputs = self.model.get_text_features(**inputs)
                embeddings = self.projection(outputs)
                embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
        
        return embeddings.float()
```

#### åŠ¹æœ
- **é€Ÿåº¦**: ç´„2å€é«˜é€ŸåŒ–
- **ãƒ¡ãƒ¢ãƒª**: ç´„50%å‰Šæ¸›
- **ç²¾åº¦**: L2æ­£è¦åŒ–ã«ã‚ˆã‚Šæ¤œç´¢ç²¾åº¦ã‚’ç¶­æŒ

#### FutureWarningä¿®æ­£
```python
# éæ¨å¥¨
with torch.cuda.amp.autocast(enabled=(self.device == "cuda")):

# æ¨å¥¨ (PyTorch 2.0+)
with torch.amp.autocast('cuda', enabled=(self.device == "cuda")):
```

#### æ•™è¨“
âœ… **FP16æ··åˆç²¾åº¦ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§æœ‰åŠ¹**
- ãƒ¢ãƒ‡ãƒ«ã®`.half()`ã§FP16å¤‰æ›
- `torch.amp.autocast`ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
- å‡ºåŠ›ã¯`.float()`ã§FP32ã«æˆ»ã—äº’æ›æ€§ç¢ºä¿
- æ­£è¦åŒ–å‡¦ç†ã¯ç²¾åº¦ç¶­æŒã«å¿…é ˆ

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

### å‡¦ç†æ™‚é–“ã®æ¨ç§»

| æœ€é©åŒ–æ®µéš | åŸ‹ã‚è¾¼ã¿æ™‚é–“ (depth 0) | Treeæ§‹ç¯‰æ™‚é–“ (æ¨å®š) | é«˜é€ŸåŒ–ç‡ |
|----------|-------------------|-----------------|--------|
| Ollama HTTP | 40åˆ†ä»¥ä¸Š | 6æ™‚é–“ä»¥ä¸Š | 1.0x (åŸºæº–) |
| HuggingFace GPU | ~7åˆ† | ~3æ™‚é–“ | 10-15x |
| + Batch=50 | ~5.5åˆ† | ~2.5æ™‚é–“ | 1.25x |
| + FP16 | ~3åˆ† | ~1.5-2æ™‚é–“ | 2x |
| **åˆè¨ˆ** | **~3åˆ†** | **~2æ™‚é–“** | **ç´„3-4x** |

### GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

| è¨­å®š | é–‹å§‹æ™‚ | ãƒ”ãƒ¼ã‚¯æ™‚ | åˆ©ç”¨ç‡ |
|-----|-------|--------|-------|
| åˆæœŸ (FP32, batch=32) | 6512 MB | ~14000 MB | 85% |
| FP16, batch=40 | 6512 MB | ~10000 MB | 61% |
| FP16, batch=50 | 6512 MB | ~11000 MB | 67% |

---

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢
- **GPU**: NVIDIA RTX 4060 Ti (16GB VRAM)
- **CUDA**: 11.8 / 12.1

### ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢
```python
# ã‚³ã‚¢ä¾å­˜é–¢ä¿‚
torch==2.7.1+cu118
torchvision==0.22.1+cu118
transformers>=4.35.0
sentence-transformers==5.1.0
langchain-huggingface==1.0.0

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
sentencepiece==0.2.1
faiss-cpu==1.12.0
```

### ãƒ¢ãƒ‡ãƒ«
1. **ColVBERT (BLIP)**
   - Vision: `Salesforce/blip-image-captioning-base`
   - Text: `intfloat/multilingual-e5-large`

2. **ColModernVBERT (SigLIP)**
   - Multimodal: `google/siglip-base-patch16-224`

3. **Text Embeddings**
   - `mixedbread-ai/mxbai-embed-large-v1`

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
```python
# âœ… ç”»åƒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨
if not images_exist:
    convert_pdfs_to_images(pdf_dir, images_dir, dpi=150)

# âœ… ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨
if text_cache_file.exists():
    page_texts = load_text_cache(text_cache_file)
```

### 2. ãƒãƒ£ãƒ³ã‚¯è¨­å®š
```python
# âœ… å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‘ã‘è¨­å®š
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,      # é•·ã‚ã«è¨­å®š
    chunk_overlap=150,   # ååˆ†ãªã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    length_function=len
)
```

### 3. GPUåŸ‹ã‚è¾¼ã¿
```python
# âœ… ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æœ€é©åŒ–
text_embeddings = HuggingFaceEmbeddings(
    model_kwargs={'device': 'cuda'},
    encode_kwargs={
        'normalize_embeddings': True,
        'batch_size': 64  # GPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´
    }
)
```

### 4. FP16æ··åˆç²¾åº¦
```python
# âœ… ãƒ¢ãƒ‡ãƒ«ã®FP16å¤‰æ›
if device == "cuda":
    model = model.half()

# âœ… autocastã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
with torch.amp.autocast('cuda', enabled=True):
    outputs = model(inputs)

# âœ… å‡ºåŠ›ã¯FP32ã«æˆ»ã™
return outputs.float()
```

### 5. Tree ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
```python
# âœ… pickleä¿å­˜
pickle.dump({
    'tree': tree,
    'build_time': time,
    'stats': stats
}, f)

# âœ… æ¡ä»¶åˆ†å²
if pickle_file.exists():
    tree = load_tree(pickle_file)
else:
    tree = build_tree()
    save_tree(tree, pickle_file)
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¨ãƒ©ãƒ¼1: Ollama Context Limit
```
Error: context length exceeded
åŸå› : ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¦ç·ãƒãƒ£ãƒ³ã‚¯æ•°ãŒå¤šã„
è§£æ±º: chunk_size=800, overlap=150ã«å¢—åŠ 
```

### ã‚¨ãƒ©ãƒ¼2: AttributeError: 'query'
```
Error: 'VisualRAPTORColBERT' object has no attribute 'query'
åŸå› : ãƒ¡ã‚½ãƒƒãƒ‰åã®èª¤ã‚Š
è§£æ±º: query() â†’ retrieve() ã«å¤‰æ›´
```

### ã‚¨ãƒ©ãƒ¼3: SentencePiece Missing
```
Error: SiglipTokenizer requires the SentencePiece library
è§£æ±º: pip install sentencepiece
```

### ã‚¨ãƒ©ãƒ¼4: CUDA Out of Memory
```
Error: CUDA out of memory
åŸå› : ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹
è§£æ±º: batch_size ã‚’ 50 â†’ 40 ã«å‰Šæ¸›
```

### è­¦å‘Š: FutureWarning autocast
```
Warning: torch.cuda.amp.autocast(args...) is deprecated
è§£æ±º: torch.amp.autocast('cuda', args...) ã«å¤‰æ›´
```

---

### Phase 8: LLMãƒ¢ãƒ‡ãƒ«é¸æŠã¨ã‚µãƒãƒªãƒ¼ç”Ÿæˆã®æœ€é©åŒ–

#### å•é¡Œ1: ã‚µãƒãƒªãƒ¼æ–‡å­—æ•°ãŒç•°å¸¸ã«å°‘ãªã„
```
å®Ÿè¡Œçµæœ:
ğŸ“¦ Cluster 0: 336 documents
   ğŸ”„ Summarizing 336 documents... âœ… Done (2 chars)

å•é¡Œ: è¦ç´„ãŒ2æ–‡å­—ã—ã‹ç”Ÿæˆã•ã‚Œãªã„
æœŸå¾…: 300-500æ–‡å­—ã®è¦ç´„
```

#### æ ¹æœ¬åŸå› åˆ†æ
```python
# å•é¡Œã®ã‚ã£ãŸè¨­å®š
llm = ChatOllama(
    model="granite-code:8b",  # âŒ ã‚³ãƒ¼ãƒ‰ç”Ÿæˆç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
    temperature=0.0           # âŒ éåº¦ã«å³æ ¼
)
```

**granite-code:8bã®ç‰¹æ€§:**
- IBMã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå°‚ç”¨ãƒ¢ãƒ‡ãƒ«
- ç”¨é€”: ã‚³ãƒ¼ãƒ‰è£œå®Œã€ãƒã‚°ä¿®æ­£ã€ã‚³ãƒ¼ãƒ‰èª¬æ˜
- **æ—¥æœ¬èªã®è‡ªç„¶è¨€èªè¦ç´„ã«ã¯ä¸é©**
- çµæœ: è¦ç´„ã‚¿ã‚¹ã‚¯ã§æœ€å°é™ã®å‡ºåŠ› (2æ–‡å­—)

#### è§£æ±ºç­–1: LLMãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´
```python
# æ”¹å–„å¾Œã®è¨­å®š
llm = ChatOllama(
    model="qwen2.5:7b",       # âœ… å¤šè¨€èªæ±ç”¨ãƒ¢ãƒ‡ãƒ«
    temperature=0.3,          # âœ… è‡ªç„¶ãªè¦ç´„ç”Ÿæˆ
    num_ctx=8192             # âœ… æ˜ç¤ºçš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
)
```

**qwen2.5:7bã®ç‰¹æ€§:**
- Alibaba Cloudã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«
- å¯¾å¿œè¨€èª: ä¸­å›½èªã€è‹±èªã€æ—¥æœ¬èªãªã©
- ç”¨é€”: æ±ç”¨NLPã‚¿ã‚¹ã‚¯ã€è¦ç´„ã€å¯¾è©±
- **æ—¥æœ¬èªè¦ç´„ã‚¿ã‚¹ã‚¯ã«æœ€é©**

#### å•é¡Œ2: ã‚µãƒãƒªãƒ¼ç”Ÿæˆæ™‚é–“ãŒé•·ã™ãã‚‹

```
å•é¡Œ: 1ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã«æ•°åˆ†ã‹ã‹ã‚‹
åŸå› : 
- å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹ (åŠ¹æœçš„ã§ãªã„)
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡ºãªã—
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³
```

#### è§£æ±ºç­–2: summarize_cluster()é–¢æ•°ã®æ”¹å–„

**æ”¹å–„å‰:**
```python
def summarize_cluster(self, documents: List[Document]) -> str:
    texts = [doc.page_content for doc in documents]
    combined_text = "\n\n".join(texts)
    
    prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ä¿æŒã—ãªãŒã‚‰ã€"
        "å…¨ä½“ã®å†…å®¹ã‚’200-300æ–‡å­—ç¨‹åº¦ã§ã¾ã¨ã‚ã¦ãã ã•ã„:\n\n{text}"
    )
    
    chain = prompt | self.llm | StrOutputParser()
    summary = chain.invoke({"text": combined_text[:4000]})
    return summary
```

**æ”¹å–„å¾Œ:**
```python
def summarize_cluster(self, documents: List[Document]) -> str:
    """ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¦ç´„(æ”¹å–„ç‰ˆ: å…¥åŠ›æ‹¡å¼µã€å“è³ªãƒã‚§ãƒƒã‚¯ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡º)"""
    texts = [doc.page_content for doc in documents]
    combined_text = "\n\n".join(texts)
    
    # 1. å…¥åŠ›é•·ã‚’æ‹¡å¼µ (4000 â†’ 8000æ–‡å­—)
    max_input_length = 8000
    
    # 2. é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¯å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if len(combined_text) > max_input_length:
        sample_ratio = max_input_length / len(combined_text)
        sampled_texts = [
            text[:int(len(text) * sample_ratio)] 
            for text in texts
        ]
        combined_text = "\n\n".join(sampled_texts)[:max_input_length]
    
    # 3. ç½å®³ç‰¹åŒ–ã®æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã¯ç½å®³æ•™è¨“ã«é–¢ã™ã‚‹è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚\n\n"
        "ã€è¦ç´„ã‚¿ã‚¹ã‚¯ã€‘\n"
        "- ä¸»è¦ãªç½å®³äº‹ä¾‹ã€æ•™è¨“ã€å¯¾ç­–ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„\n"
        "- 300-500æ–‡å­—ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„\n"
        "- ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ®µè½å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„\n\n"
        "ã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{text}\n\nã€è¦ç´„ã€‘"
    )
    
    chain = prompt | self.llm | StrOutputParser()
    
    # 4. å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬
    start_time = time.time()
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            summary = chain.invoke({"text": combined_text})
            
            # 5. å“è³ªãƒã‚§ãƒƒã‚¯: çŸ­ã™ãã‚‹è¦ç´„ã‚’æ¤œå‡º
            if len(summary) < 50:
                print(f"âš ï¸ Short summary ({len(summary)} chars), retrying...")
                continue
            
            # 6. å®Ÿè¡Œæ™‚é–“ã‚’è¡¨ç¤º
            elapsed = time.time() - start_time
            print(f"âœ… Done ({len(summary)} chars, {elapsed:.1f}s)")
            return summary
            
        except Exception as e:
            print(f"âš ï¸ Summarization error (attempt {attempt+1}): {e}")
            if attempt == max_retries - 1:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®1000æ–‡å­—ã‚’è¿”ã™
                print("âš ï¸ Falling back to truncated text")
                return combined_text[:1000] + "..."
    
    return combined_text[:1000] + "..."
```

#### æ”¹å–„å†…å®¹ã®è©³ç´°

**1. å…¥åŠ›é•·ã®æ‹¡å¼µã¨å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**
```python
# Before: 4000æ–‡å­—ã§åˆ‡ã‚Šæ¨ã¦ (æƒ…å ±æå¤±)
combined_text[:4000]

# After: 8000æ–‡å­—ã¾ã§æ‹¡å¼µã€é•·ã„å ´åˆã¯å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
if len(combined_text) > 8000:
    sample_ratio = 8000 / len(combined_text)
    # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å‡ç­‰ã«æŠ½å‡º
    sampled_texts = [text[:int(len(text) * sample_ratio)] for text in texts]
```

**2. ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**
```python
# Before: æ±ç”¨çš„ãªè¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"

# After: ç½å®³æ•™è¨“ã«ç‰¹åŒ–ã—ãŸæ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
"""
ã€è¦ç´„ã‚¿ã‚¹ã‚¯ã€‘
- ä¸»è¦ãªç½å®³äº‹ä¾‹ã€æ•™è¨“ã€å¯¾ç­–ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
- 300-500æ–‡å­—ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„
- ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ®µè½å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„
"""
```

**3. å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹**
```python
# çŸ­ã™ãã‚‹è¦ç´„ã‚’æ¤œå‡ºã—ã¦ãƒªãƒˆãƒ©ã‚¤
if len(summary) < 50:
    print(f"âš ï¸ Short summary ({len(summary)} chars), retrying...")
    continue

# æœ€å¤§3å›ã¾ã§å†è©¦è¡Œ
max_retries = 3
```

**4. å®Ÿè¡Œæ™‚é–“ã®å¯è¦–åŒ–**
```python
start_time = time.time()
# ... å‡¦ç† ...
elapsed = time.time() - start_time
print(f"âœ… Done ({len(summary)} chars, {elapsed:.1f}s)")

# å‡ºåŠ›ä¾‹:
# âœ… Done (387 chars, 8.5s)
```

**5. å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**
```python
try:
    summary = chain.invoke({"text": combined_text})
except Exception as e:
    print(f"âš ï¸ Summarization error: {e}")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã¦è¿”ã™
    return combined_text[:1000] + "..."
```

#### å®Ÿè¡Œçµæœã®æ¯”è¼ƒ

**Before (granite-code:8b):**
```
ğŸ“¦ Cluster 0: 336 documents
   ğŸ”„ Summarizing 336 documents... âœ… Done (2 chars)
ğŸ“¦ Cluster 1: 284 documents
   ğŸ”„ Summarizing 284 documents... âœ… Done (2 chars)

å•é¡Œ:
- è¦ç´„ãŒ2æ–‡å­—ã®ã¿ (ã»ã¼ç„¡æ„å‘³)
- å‡¦ç†æ™‚é–“ãŒè¡¨ç¤ºã•ã‚Œãªã„
- ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œãªã„
```

**After (qwen2.5:7b + æ”¹å–„ç‰ˆé–¢æ•°):**
```
ğŸ“¦ Cluster 0: 336 documents
   ğŸ”„ Summarizing 336 documents... âœ… Done (387 chars, 8.5s)
ğŸ“¦ Cluster 1: 284 documents
   ğŸ”„ Summarizing 284 documents... âœ… Done (452 chars, 7.2s)

æ”¹å–„:
- 300-500æ–‡å­—ã®å®Ÿè³ªçš„ãªè¦ç´„
- å‡¦ç†æ™‚é–“ãŒæ˜ç¢º (5-15ç§’/ã‚¯ãƒ©ã‚¹ã‚¿)
- å“è³ªãƒã‚§ãƒƒã‚¯ã§ç•°å¸¸æ¤œå‡º
```

#### æ•™è¨“

âœ… **LLMãƒ¢ãƒ‡ãƒ«é¸æŠã¯ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦æ…é‡ã«**
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ¢ãƒ‡ãƒ« â‰  è‡ªç„¶è¨€èªè¦ç´„ãƒ¢ãƒ‡ãƒ«
- granite-code â†’ ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯å°‚ç”¨
- qwen2.5 â†’ å¤šè¨€èªNLPã‚¿ã‚¹ã‚¯å…¨èˆ¬

âœ… **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®é‡è¦æ€§**
- æ±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- å‡ºåŠ›å½¢å¼ã‚’æ˜ç¢ºã«æŒ‡ç¤º (æ®µè½å½¢å¼ã€æ–‡å­—æ•°ç¯„å›²)
- ã‚¿ã‚¹ã‚¯ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ˜ç¤º

âœ… **å“è³ªä¿è¨¼ã®ä»•çµ„ã¿ã‚’çµ„ã¿è¾¼ã‚€**
- å‡ºåŠ›ã®é•·ã•ãƒã‚§ãƒƒã‚¯
- ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

âœ… **å¯è¦³æ¸¬æ€§ã®å‘ä¸Š**
- å®Ÿè¡Œæ™‚é–“ã®è¨ˆæ¸¬ã¨è¡¨ç¤º
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°åŒ–
- å‡¦ç†çŠ¶æ…‹ã®å¯è¦–åŒ–

âœ… **Temperatureè¨­å®šã®æœ€é©åŒ–**
- 0.0: éåº¦ã«æ±ºå®šè«–çš„ (å‰µé€ æ€§ãªã—)
- 0.3: è‡ªç„¶ã§æµæš¢ãªè¦ç´„ç”Ÿæˆ
- 0.7+: å‰µé€ çš„ã ãŒä¸€è²«æ€§ãŒä½ä¸‹

#### ã‚³ãƒ¼ãƒ‰æ¯”è¼ƒè¡¨

| é …ç›® | Before | After | æ”¹å–„åŠ¹æœ |
|------|--------|-------|---------|
| **LLMãƒ¢ãƒ‡ãƒ«** | granite-code:8b | qwen2.5:7b | æ—¥æœ¬èªè¦ç´„ã«æœ€é©åŒ– |
| **Temperature** | 0.0 | 0.3 | è‡ªç„¶ãªæ–‡ç« ç”Ÿæˆ |
| **å…¥åŠ›é•·** | 4000æ–‡å­— | 8000æ–‡å­— | 2å€ã®æƒ…å ±é‡ |
| **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | åˆ‡ã‚Šæ¨ã¦ | å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | æƒ…å ±æå¤±ã®è»½æ¸› |
| **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ** | æ±ç”¨ | ç½å®³ç‰¹åŒ– | çš„ç¢ºãªè¦ç´„ |
| **ç›®æ¨™æ–‡å­—æ•°** | 200-300 | 300-500 | ã‚ˆã‚Šè©³ç´°ãªè¦ç´„ |
| **å“è³ªãƒã‚§ãƒƒã‚¯** | ãªã— | <50æ–‡å­—æ¤œå‡º | ç•°å¸¸æ¤œå‡º |
| **ãƒªãƒˆãƒ©ã‚¤** | ãªã— | æœ€å¤§3å› | æˆåŠŸç‡å‘ä¸Š |
| **æ™‚é–“è¨ˆæ¸¬** | ãªã— | è¡¨ç¤ºã‚ã‚Š | å¯è¦³æ¸¬æ€§å‘ä¸Š |
| **ã‚¨ãƒ©ãƒ¼å‡¦ç†** | åŸºæœ¬çš„ | è©³ç´°+ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ | å …ç‰¢æ€§å‘ä¸Š |
| **å‡ºåŠ›æ–‡å­—æ•°** | 2 chars | 300-500 chars | **150-250å€** |

---

## ä»Šå¾Œã®å±•é–‹

### ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã®å¯èƒ½æ€§

1. **Dynamic Batching**
   - ç”»åƒã‚µã‚¤ã‚ºã«å¿œã˜ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´

2. **Gradient Checkpointing**
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ã•ã‚‰ã«æ”¹å–„

3. **åˆ†æ•£å‡¦ç†**
   - è¤‡æ•°GPUã§ã®ä¸¦åˆ—Treeæ§‹ç¯‰

4. **é‡å­åŒ– (INT8)**
   - FP16ã‚ˆã‚Šã•ã‚‰ã«é«˜é€ŸåŒ–

5. **LLMã®è»½é‡åŒ–**
   - qwen2.5:7b â†’ qwen2.5:3b (é€Ÿåº¦å„ªå…ˆã®å ´åˆ)
   - LoRAã‚„QLoRAã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### è©•ä¾¡é …ç›®

1. **æ¤œç´¢ç²¾åº¦**
   - Precision@K, Recall@K
   - NDCG (Normalized Discounted Cumulative Gain)

2. **Treeå“è³ª**
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ª (Silhouette Score)
   - Treeæ·±ã•ã¨å¹…ã®ãƒãƒ©ãƒ³ã‚¹
   - **è¦ç´„ã®æƒ…å ±å¯†åº¦ã¨èª­ã¿ã‚„ã™ã•**

3. **å®Ÿç”¨æ€§**
   - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å¿œç­”æ™‚é–“
   - ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆ
   - **è¦ç´„ç”Ÿæˆã®é€Ÿåº¦ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹**

---

## ã¾ã¨ã‚

### æˆåŠŸè¦å› 
âœ… ãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ–ã§28%ã®ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›  
âœ… GPUåŸ‹ã‚è¾¼ã¿ã§10-15å€é«˜é€ŸåŒ–  
âœ… FP16æ··åˆç²¾åº¦ã§2å€é«˜é€ŸåŒ–ã¨ãƒ¡ãƒ¢ãƒª50%å‰Šæ¸›  
âœ… Tree ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã§å†æ§‹ç¯‰æ™‚é–“ã‚¼ãƒ­  
âœ… ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ã®æ­£ç¢ºãªçµ±åˆ  
âœ… **LLMãƒ¢ãƒ‡ãƒ«ã®é©åˆ‡ãªé¸æŠã§è¦ç´„å“è³ªãŒ150-250å€å‘ä¸Š** â­NEW

### æœ€çµ‚çš„ãªæ€§èƒ½
- **å‡¦ç†æ™‚é–“**: 6æ™‚é–“ä»¥ä¸Š â†’ **ç´„2æ™‚é–“** (3-4å€é«˜é€ŸåŒ–)
- **GPUãƒ¡ãƒ¢ãƒª**: 85%åˆ©ç”¨ â†’ **61%åˆ©ç”¨** (24%å‰Šæ¸›)
- **å†ç¾æ€§**: pickle ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§å³åº§ã«å†åˆ©ç”¨å¯èƒ½
- **è¦ç´„å“è³ª**: 2æ–‡å­— â†’ **300-500æ–‡å­—** (150-250å€æ”¹å–„) â­NEW
- **è¦ç´„é€Ÿåº¦**: æ˜ç¢ºãªæ™‚é–“è¨ˆæ¸¬ (**5-15ç§’/ã‚¯ãƒ©ã‚¹ã‚¿**) â­NEW

### é‡è¦ãªæ•™è¨“
1. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®æœ€é©åŒ–ãŒå¿…é ˆ**
2. **GPUæ´»ç”¨ã¨HTTPå›é¿ãŒæœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ¶ˆ**
3. **FP16ã¯é€Ÿåº¦ã¨ãƒ¡ãƒ¢ãƒªã®ä¸¡æ–¹ã§æœ‰åŠ¹**
4. **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ã¯é–‹ç™ºåŠ¹ç‡ã‚’åŠ‡çš„ã«æ”¹å–„**
5. **ç¶™æ‰¿é–¢ä¿‚ã®æ­£ç¢ºãªç†è§£ãŒãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆã®éµ**
6. **LLMãƒ¢ãƒ‡ãƒ«é¸æŠã¯ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã«åˆã‚ã›ã‚‹ (ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ vs è‡ªç„¶è¨€èªè¦ç´„)** â­NEW
7. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§å“è³ªã¨å …ç‰¢æ€§ã‚’ç¢ºä¿** â­NEW

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚ºä¸€è¦§

| Phase | èª²é¡Œ | è§£æ±ºç­– | æˆæœ |
|-------|------|--------|------|
| 1 | Ollama context limit | ãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ– (800/150) | 28%å‰Šæ¸› |
| 2 | ç”»åƒåŸ‹ã‚è¾¼ã¿æœªä½¿ç”¨ | visual_encoderçµ±åˆ | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æœ‰åŠ¹åŒ– |
| 3 | HTTPåŸ‹ã‚è¾¼ã¿é…å»¶ | HuggingFace GPUåŒ– | 10-15å€é«˜é€ŸåŒ– |
| 4 | query()ãƒ¡ã‚½ãƒƒãƒ‰ã‚¨ãƒ©ãƒ¼ | retrieve()ã«ä¿®æ­£ | APIæ­£å¸¸åŒ– |
| 5 | Treeå†æ§‹ç¯‰ã®æ™‚é–“æå¤± | pickleã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° | å†æ§‹ç¯‰0ç§’ |
| 6 | ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´ | 40 embeddings, 64 text | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€é©åŒ– |
| 7 | ãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒ”ãƒ¼ãƒ‰ | FP16æ··åˆç²¾åº¦ | 2å€é«˜é€Ÿã€50%ãƒ¡ãƒ¢ãƒªå‰Šæ¸› |
| **8** | **è¦ç´„å“è³ªã¨é€Ÿåº¦** | **LLMå¤‰æ›´+é–¢æ•°æ”¹å–„** | **150-250å€å“è³ªå‘ä¸Š** |
| **9** | **å®Ÿè¡Œæ™‚é–“ã¨ãƒªã‚¹ã‚¯ç®¡ç†** | **å˜ä¸€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€æˆ¦ç•¥** | **50%æ™‚é–“å‰Šæ¸›ã€ãƒ‡ãƒ¼ã‚¿ä¿è­·** â­NEW |

---

## Phase 9: æˆ¦ç•¥çš„ç„¦ç‚¹ - ColVBERT (BLIP)å°‚ç”¨åŒ–

### èƒŒæ™¯ã¨èª²é¡Œ

#### å•é¡Œ1: 12æ™‚é–“è¶…ã®å®Ÿè¡Œæ™‚é–“

```
å®Ÿè¡Œè¨ˆç”»: ColVBERT + ColModernVBERT ã®ä¸¡æ–¹ã‚’46 PDFsã§æ¯”è¼ƒ
æ¨å®šæ™‚é–“: å„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ ~12æ™‚é–“ = åˆè¨ˆ24æ™‚é–“ä»¥ä¸Š
ãƒªã‚¹ã‚¯: 
- é•·æ™‚é–“å®Ÿè¡Œä¸­ã®ä¸­æ–­ãƒªã‚¹ã‚¯
- GPUãƒªã‚½ãƒ¼ã‚¹ã®éåŠ¹ç‡çš„ãªä½¿ç”¨
- ãƒ‡ãƒãƒƒã‚°ã‚µã‚¤ã‚¯ãƒ«ã®é•·æœŸåŒ–
```

**è¦³æ¸¬çµæœ:**
```
å®Ÿè¡Œæ™‚é–“: 12æ™‚é–“ä»¥ä¸ŠçµŒé
GPUä½¿ç”¨ç‡: å¹³å‡70-85%
é€²æ—: ColVBERT Treeæ§‹ç¯‰ã¾ã§å®Œäº†
èª²é¡Œ: æ¬¡ã®ColModernVBERTæ§‹ç¯‰ã§ã•ã‚‰ã«12æ™‚é–“å¿…è¦
```

#### å•é¡Œ2: Treeå‰Šé™¤ã«ã‚ˆã‚‹5æ™‚é–“ã®æå¤±

```
ç™ºç”Ÿäº‹è±¡: colbert_blip_tree_46pdfs_chunked.pkl ã®å‰Šé™¤
å½±éŸ¿:
- æ§‹ç¯‰æ™‚é–“: 18,193.5ç§’ (5æ™‚é–“3åˆ†) ã®è¨ˆç®—ãŒç„¡é§„ã«
- Treeæ§‹é€ : 27ãƒãƒ¼ãƒ‰ (ãƒªãƒ¼ãƒ•18, å†…éƒ¨9, æ·±åº¦3) å…¨æå¤±
- æ®‹å­˜ãƒ‡ãƒ¼ã‚¿: JSONãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ (treeæ§‹é€ ãªã—)
```

**å‰Šé™¤ã•ã‚ŒãŸTreeã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:**
```json
{
  "build_time": 18193.55,  // 5æ™‚é–“3åˆ†
  "stats": {
    "num_leaf_nodes": 18,
    "num_internal_nodes": 9,
    "total_nodes": 27,
    "max_depth": 3
  },
  "num_chunks": 4250,
  "note": "Tree structure saved (Document objects not serializable)"
}
```

### æˆ¦ç•¥çš„åˆ¤æ–­

#### æ±ºå®š: ColVBERT (BLIP)ã®ã¿ã«é›†ä¸­

**ç†ç”±:**

1. **æ™‚é–“å‰Šæ¸›**: 24æ™‚é–“ â†’ 12æ™‚é–“ (50%å‰Šæ¸›)
2. **ãƒªã‚¹ã‚¯ç®¡ç†**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§å¤±æ•—ãƒã‚¤ãƒ³ãƒˆå‰Šæ¸›
3. **å®Ÿç”¨æ€§**: 1ã¤ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ååˆ†ãªæ€§èƒ½ã‚’é”æˆã§ãã‚Œã°ã€æ¯”è¼ƒã¯å¾Œå›ã—ã§ã‚‚è‰¯ã„
4. **é–‹ç™ºåŠ¹ç‡**: ãƒ‡ãƒãƒƒã‚°ã¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚¯ãƒ«ã®é«˜é€ŸåŒ–

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ±ºå®š:**
> ã€Œ46pdfã®è¨ˆç®—ãŒ12æ™‚é–“ã‚’ã“ãˆãŸ...treeãŒå‰Šé™¤ã•ã‚Œã¦é•·æ™‚é–“ã®è¨ˆç®—ãŒç„¡é§„ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã—ãŸã„ã€  
> ã€Œï¼’ã¤å®Ÿè¡Œã™ã‚‹ã“ã¨ã¯æ¬²å¼µã‚Šã™ãã¦ã„ãŸ...ColVBERT (BLIP)ã®ã¿ã«é›†ä¸­ã€

### å®Ÿè£…: build_ColVBERT_BLIP_tree_46pdfs.py

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ç‰¹å¾´

```python
"""
ColVBERT (BLIP) RAPTOR Treeæ§‹ç¯‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - 46 PDFæ–‡æ›¸ç‰ˆ

æ—¢å­˜ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦:
1. ãƒ†ã‚­ã‚¹ãƒˆã‚’800ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
2. ColVBERT (BLIP)ã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
3. RAPTORéšå±¤ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰
4. Treeã‚’pickleå½¢å¼ã§ä¿å­˜ï¼ˆå¾©å…ƒå¯èƒ½ï¼‰

ã€é‡è¦ã€‘
- Treeã¯è‡ªå‹•çš„ã«pickleä¿å­˜ã•ã‚Œã€æ¬¡å›å®Ÿè¡Œæ™‚ã¯å³åº§ã«ãƒ­ãƒ¼ãƒ‰
- 12æ™‚é–“ä»¥ä¸Šã®è¨ˆç®—ã‚’ä¿è­·ã™ã‚‹ãŸã‚ã€ã“ã¾ã‚ãªä¿å­˜ã‚’å®Ÿæ–½
"""
```

#### ä¸»è¦ãªè¨­è¨ˆæ±ºå®š

**1. ä¾å­˜é–¢ä¿‚ã®ä¿®æ­£**

```python
# ä¿®æ­£å‰ (tsunami_lesson_raptor.py):
sys.path.append(str(Path(__file__).parent / "raptor-faiss-kmean-cluster-eval"))
from raptor_eval import RAPTORRetrieverEval

# ä¿®æ­£å¾Œ:
from raptor_eval import RAPTORRetrieverEval  # åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®GPUæœ€é©åŒ–ç‰ˆ
```

**ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒ¼ãƒ³:**
```
build_ColVBERT_BLIP_tree_46pdfs.py
  â†’ visual_raptor_colbert.py
    â†’ 0_base_tsunami-lesson-rag/tsunami_lesson_raptor.py
      â†’ 0_base_tsunami-lesson-rag/raptor_eval.py (GPUæœ€é©åŒ–ç‰ˆ) âœ…
```

**2. å˜ä¸€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ã¿**

```python
# ColVBERT (BLIP)ã®ã¿è¨­å®š
colbert_config = {
    'encoder_type': 'colbert',
    'text_model': 'intfloat/multilingual-e5-large',
    'vision_model': 'Salesforce/blip-image-captioning-base',
    'embedding_dim': 768,
    'use_cross_attention': False
}

colbert_system = VisualRAPTORColBERT(
    embeddings_model=embeddings_model,
    llm=llm,
    use_modern_vbert=False,  # ColModernVBERTç„¡åŠ¹åŒ–
    colbert_config=colbert_config,
    pdf_source_dir=str(pdf_source_dir)
)
```

**3. Treeä¿è­·ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **

```python
# Treeä¿å­˜ï¼ˆpickleï¼‰
with open(colbert_tree_pickle, 'wb') as f:
    pickle.dump({
        'tree': colbert_tree,
        'build_time': colbert_tree_build_time,
        'stats': colbert_tree_stats,
        'timestamp': datetime.now().isoformat()  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¿½åŠ 
    }, f)

# çµ±è¨ˆæƒ…å ±ä¿å­˜ï¼ˆJSONï¼‰- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦
with open(colbert_tree_file, 'w', encoding='utf-8') as f:
    json.dump({
        'build_time': colbert_tree_build_time,
        'build_time_hours': colbert_tree_build_time / 3600,
        'stats': colbert_tree_stats,
        'num_chunks': len(visual_documents),
        'num_pages': len(image_files),
        'chunk_size': CHUNK_SIZE,
        'chunk_overlap': CHUNK_OVERLAP,
        'timestamp': datetime.now().isoformat(),
        'note': 'Tree structure saved in .pkl file'
    }, f, indent=2, ensure_ascii=False)
```

**4. Treeå†åˆ©ç”¨æ©Ÿèƒ½**

```python
if colbert_tree_pickle.exists():
    print(f"ğŸ“‚ æ—¢å­˜ã®ColVBERT Treeã‚’èª­ã¿è¾¼ã¿ä¸­: {colbert_tree_pickle.name}")
    try:
        import pickle
        with open(colbert_tree_pickle, 'rb') as f:
            tree_data = pickle.load(f)
            colbert_tree = tree_data['tree']
            colbert_tree_build_time = tree_data['build_time']
            colbert_tree_stats = tree_data['stats']
        print(f"âœ… ColVBERT Treeèª­ã¿è¾¼ã¿å®Œäº†")
        print(f"  æ§‹ç¯‰æ™‚é–“ (å‰å›): {colbert_tree_build_time:.2f}ç§’ ({colbert_tree_build_time/60:.1f}åˆ†)")
        colbert_system.tree = colbert_tree  # ã‚·ã‚¹ãƒ†ãƒ ã«è¨­å®š
    except Exception as e:
        print(f"âš ï¸ Treeèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        colbert_tree = None
```

### å®Ÿè¡Œçµæœï¼ˆé€²è¡Œä¸­ï¼‰

```
================================================================================
ColVBERT (BLIP) RAPTOR Treeæ§‹ç¯‰
46 PDFæ–‡æ›¸ã€2378ãƒšãƒ¼ã‚¸ â†’ 4250ãƒãƒ£ãƒ³ã‚¯
================================================================================

[ã‚¹ãƒ†ãƒƒãƒ— 1/5] ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™...
âœ… data\encoder_comparison_46pdfs\results æº–å‚™å®Œäº†
âœ… data\encoder_comparison_46pdfs\raptor_trees æº–å‚™å®Œäº†

[ã‚¹ãƒ†ãƒƒãƒ— 2/5] æ—¢å­˜ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ä¸­...
âœ… 2378æšã®ç”»åƒã‚’ç™ºè¦‹
âœ… ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿: 2378ã‚¨ãƒ³ãƒˆãƒª

[ã‚¹ãƒ†ãƒƒãƒ— 3/5] 2378å€‹ã®VisualDocumentã‚’ä½œæˆã—ã¦ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸­...
âœ… 2378ãƒšãƒ¼ã‚¸ã‹ã‚‰4250å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆå®Œäº†
   å¹³å‡ãƒãƒ£ãƒ³ã‚¯æ•°/ãƒšãƒ¼ã‚¸: 1.8

================================================================================
ColVBERT (BLIP) ã§RAPTOR Treeæ§‹ç¯‰
================================================================================

[ã‚¹ãƒ†ãƒƒãƒ— 4/5] ColVBERT (BLIP) ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...
  Using device: cuda
  GPUçŠ¶æ…‹ (é–‹å§‹å‰): XXXX MB / XXXX MB
âœ… ColVBERTåˆæœŸåŒ–å®Œäº†

[ã‚¹ãƒ†ãƒƒãƒ— 5/5] ğŸŒ³ æ–°è¦ã«RAPTOR Treeã‚’æ§‹ç¯‰ä¸­...
  å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯æ•°: 4250
  âš ï¸ æ¨å®šæ™‚é–“: 3-5æ™‚é–“ï¼ˆGPUæœ€é©åŒ–ç‰ˆï¼‰
  ğŸ’¾ è‡ªå‹•ä¿å­˜: colbert_blip_tree_46pdfs_chunked.pkl

[é€²è¡Œä¸­...]
```

### æœŸå¾…ã•ã‚Œã‚‹æˆæœ

#### 1. å®Ÿè¡Œæ™‚é–“ã®çŸ­ç¸®

- **å¾“æ¥**: ColVBERT + ColModernVBERT = 24æ™‚é–“ä»¥ä¸Š
- **ç¾åœ¨**: ColVBERT ã®ã¿ = 5-6æ™‚é–“ï¼ˆGPUæœ€é©åŒ–ç‰ˆï¼‰
- **å‰Šæ¸›ç‡**: 50-75%å‰Šæ¸›

#### 2. GPUæœ€é©åŒ–ã®æ©æµ

ã™ã¹ã¦ã®Phase 1-8ã®æœ€é©åŒ–ãŒé©ç”¨:
- âœ… ãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ– (800/150)
- âœ… GPUä¸¦åˆ—å‡¦ç† (batch=64)
- âœ… FP16æ··åˆç²¾åº¦
- âœ… GPU-based multimodal fusionï¼ˆ30-40%é«˜é€ŸåŒ–ï¼‰
- âœ… qwen2.5:7bé«˜å“è³ªã‚µãƒãƒªãƒ¼ï¼ˆ300-500æ–‡å­—ï¼‰
- âœ… Treeã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆå†å®Ÿè¡Œ0ç§’ï¼‰

#### 3. ãƒ‡ãƒ¼ã‚¿ä¿è­·

- **Pickleä¿å­˜**: Treeæ§‹é€ ã€åŸ‹ã‚è¾¼ã¿ã€ã‚µãƒãƒªãƒ¼å…¨ä¿å­˜
- **JSONçµ±è¨ˆ**: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- **ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: æ§‹ç¯‰æ™‚åˆ»ã®è¨˜éŒ²
- **å†åˆ©ç”¨æ©Ÿèƒ½**: æ¬¡å›å®Ÿè¡Œæ™‚ã¯å³åº§ã«ãƒ­ãƒ¼ãƒ‰

### ColVBERT (BLIP) é¸æŠã®æ ¹æ‹ 

#### æŠ€è¡“çš„ç‰¹æ€§

**ColVBERT (BLIP):**
- Vision Model: Salesforce/blip-image-captioning-base
- ç‰¹å¾´: ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã«ç‰¹åŒ–
- å¼·ã¿: OCRãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆã«å„ªã‚Œã‚‹
- ç”¨é€”: æ–‡æ›¸ç”»åƒã€ã‚¹ã‚­ãƒ£ãƒ³è³‡æ–™ã€è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ

**ColModernVBERT (SigLIP):**
- Vision Model: google/siglip-base-patch16-224
- ç‰¹å¾´: ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®å¯¾ç…§å­¦ç¿’
- å¼·ã¿: æ±ç”¨çš„ãªç”»åƒç†è§£
- ç”¨é€”: è‡ªç„¶ç”»åƒã€å†™çœŸã€ä¸€èˆ¬çš„ãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ¤œç´¢

#### ç½å®³æ–‡æ›¸ã«å¯¾ã™ã‚‹é©åˆæ€§

46 PDFæ–‡æ›¸ã®ç‰¹æ€§:
- ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚ŒãŸå…¬çš„è³‡æ–™ãŒå¤šã„
- è¡¨ã€ã‚°ãƒ©ãƒ•ã€å›³è¡¨ãŒè±Šå¯Œ
- OCRãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆãŒé‡è¦
- **BLIPã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆèƒ½åŠ›ãŒæœ‰åˆ©**

#### å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹ã®é¸æŠ

- âœ… 2 PDFs (131ãƒšãƒ¼ã‚¸)ã§ä¸¡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®æ¤œè¨¼æ¸ˆã¿
- âœ… ColVBERTã§è‰¯å¥½ãªæ€§èƒ½ã‚’ç¢ºèª
- âœ… ã¾ãšã¯ç¢ºå®Ÿãªé¸æŠè‚¢ã§46 PDFsã‚’ã‚¹ã‚±ãƒ¼ãƒ«
- ğŸ”„ ColModernVBERTã¯å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¿½åŠ æ¤œè¨¼å¯èƒ½

### ä»Šå¾Œã®å±•é–‹

#### çŸ­æœŸï¼ˆPhase 9å®Œäº†å¾Œï¼‰

1. **ColVBERT Treeå®Œæˆ**
   - 5-6æ™‚é–“ã®å®Ÿè¡Œå®Œäº†
   - Treeä¿å­˜ç¢ºèª
   - æ¤œç´¢æ€§èƒ½è©•ä¾¡

2. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°**
   - æœ€çµ‚çš„ãªå®Ÿè¡Œæ™‚é–“è¨˜éŒ²
   - GPUä½¿ç”¨ç‡ã®ãƒ”ãƒ¼ã‚¯å€¤
   - Treeæ§‹é€ ã®è©³ç´°çµ±è¨ˆ

3. **æ¤œç´¢ç²¾åº¦è©•ä¾¡**
   - Precision@K, Recall@K
   - NDCGæ¸¬å®š
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

#### å°†æ¥çš„ãªæ‹¡å¼µå€™è£œ

1. **BLIP-2ã¸ã®ç§»è¡Œæ¤œè¨**
   - æ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ«: Salesforce/blip2-opt-2.7b / blip2-flan-t5-xl
   - æ”¹å–„ç‚¹:
     * Q-Former ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹è¦–è¦šè¨€èªçµ±åˆã®å¼·åŒ–
     * ã‚ˆã‚Šå¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆ129Mç”»åƒï¼‰
     * ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã®å‘ä¸Š
     * ã‚ˆã‚Šé•·ã„ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨VQAæ€§èƒ½
   - èª²é¡Œ:
     * æ¨è«–é€Ÿåº¦ã®ä½ä¸‹ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºå¢—åŠ ï¼‰
     * GPU ãƒ¡ãƒ¢ãƒªè¦ä»¶ã®å¢—åŠ 
     * ç¾è¡ŒBLIPã¨ã®æ€§èƒ½æ¯”è¼ƒãŒå¿…è¦
   - æ¤œè¨¼è¨ˆç”»:
     * å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ5-10 PDFsï¼‰ã§äºˆå‚™è©•ä¾¡
     * åŸ‹ã‚è¾¼ã¿å“è³ªã¨Treeæ§‹ç¯‰æ™‚é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ
     * æ¤œç´¢ç²¾åº¦ã®å®šé‡çš„æ¯”è¼ƒ

2. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–**
   - ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å‹•çš„èª¿æ•´
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€å°åŒ–
   - æ¨è«–é€Ÿåº¦ã¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹æœ€é©åŒ–

---

**ä½œæˆæ—¥**: 2025å¹´10æœˆ24æ—¥  
**æœ€çµ‚æ›´æ–°**: 2025å¹´10æœˆ25æ—¥ (Phase 9: ColVBERTå°‚ç”¨åŒ–è¿½åŠ ) â­NEW  
**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: Visual RAPTOR ColBERT  
**ãƒ‰ãƒ¡ã‚¤ãƒ³**: æ´¥æ³¢æ•™è¨“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹  
