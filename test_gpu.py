#!/usr/bin/env python3
"""
GPUå‹•ä½œãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Visual RAPTOR ColBERT ã‚·ã‚¹ãƒ†ãƒ ã®GPUæ€§èƒ½ã‚’æ¤œè¨¼

ä½¿ç”¨æ–¹æ³•:
python test_gpu.py
"""

import torch
import time
from PIL import Image
import numpy as np
from pathlib import Path

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from visual_raptor_colbert import ColVBERTEncoder

def test_gpu_availability():
    """GPUåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=" * 60)
    print("ğŸ” GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯")
    print("=" * 60)
    
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU count: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  Memory: {props.total_memory / 1024**3:.1f} GB")
            print(f"  Compute Capability: {props.major}.{props.minor}")
    
    print()

def test_colbert_gpu():
    """ColBERT ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®GPUæ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("=" * 60)
    print("ğŸš€ ColBERT GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
    encoder = ColVBERTEncoder(
        text_model_name="intfloat/multilingual-e5-large",
        vision_model_name="Salesforce/blip-image-captioning-base", 
        embedding_dim=768,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    print(f"\nğŸ“Š åˆæœŸGPUçŠ¶æ…‹:")
    gpu_info = encoder.get_gpu_memory_info()
    for key, value in gpu_info.items():
        if key.endswith('_gb'):
            print(f"  {key}: {value:.2f} GB")
        elif key.endswith('_percent'):
            print(f"  {key}: {value:.1f}%")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    test_texts = [
        "æ´¥æ³¢é¿é›£çµŒè·¯ã®ç¢ºèª",
        "ç½å®³æ™‚ã®æƒ…å ±åé›†æ–¹æ³•", 
        "ç·Šæ€¥é¿é›£æ‰€ã®å ´æ‰€",
        "é˜²ç½ã‚°ãƒƒã‚ºã®æº–å‚™",
        "å®¶æ—ã¨ã®é€£çµ¡æ‰‹æ®µ"
    ] * 10  # 50ãƒ†ã‚­ã‚¹ãƒˆ
    
    # ãƒ†ã‚¹ãƒˆç”»åƒä½œæˆ
    test_images = []
    for i in range(20):
        # ãƒ©ãƒ³ãƒ€ãƒ ãªè‰²ã®ç”»åƒã‚’ç”Ÿæˆ
        img_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        test_images.append(Image.fromarray(img_array))
    
    print(f"\nğŸ”¥ æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ:")
    print(f"  ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(test_texts)}")
    print(f"  ç”»åƒæ•°: {len(test_images)}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    text_embeddings = encoder.encode_text(test_texts, batch_size=16)
    text_time = time.time() - start_time
    
    print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœ:")
    print(f"  å‡¦ç†æ™‚é–“: {text_time:.2f}ç§’")
    print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(test_texts)/text_time:.1f} texts/sec")
    print(f"  å‡ºåŠ›å½¢çŠ¶: {text_embeddings.shape}")
    print(f"  ãƒ‡ãƒã‚¤ã‚¹: {text_embeddings.device}")
    
    # GPUçŠ¶æ…‹ç¢ºèª
    gpu_info = encoder.get_gpu_memory_info()
    print(f"  GPUä½¿ç”¨ç‡: {gpu_info.get('utilization_percent', 0):.1f}%")
    
    # ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    image_embeddings = encoder.encode_image(test_images, batch_size=8)
    image_time = time.time() - start_time
    
    print(f"\nğŸ–¼ï¸ ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœ:")
    print(f"  å‡¦ç†æ™‚é–“: {image_time:.2f}ç§’")
    print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(test_images)/image_time:.1f} images/sec")
    print(f"  å‡ºåŠ›å½¢çŠ¶: {image_embeddings.shape}")
    print(f"  ãƒ‡ãƒã‚¤ã‚¹: {image_embeddings.device}")
    
    # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    multimodal_embeddings = encoder.encode_multimodal(
        test_texts[:len(test_images)], 
        test_images
    )
    multimodal_time = time.time() - start_time
    
    print(f"\nğŸ”— ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœ:")
    print(f"  å‡¦ç†æ™‚é–“: {multimodal_time:.2f}ç§’")
    print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(test_images)/multimodal_time:.1f} pairs/sec")
    print(f"  å‡ºåŠ›å½¢çŠ¶: {multimodal_embeddings.shape}")
    
    # æœ€çµ‚GPUçŠ¶æ…‹
    print(f"\nğŸ“Š æœ€çµ‚GPUçŠ¶æ…‹:")
    gpu_info = encoder.get_gpu_memory_info()
    for key, value in gpu_info.items():
        if key.endswith('_gb'):
            print(f"  {key}: {value:.2f} GB")
        elif key.endswith('_percent'):
            print(f"  {key}: {value:.1f}%")
    
    # GPU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
    encoder.clear_gpu_cache()
    
    print(f"\nâœ… GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆå®Œäº†!")

def benchmark_cpu_vs_gpu():
    """CPU vs GPUæ€§èƒ½æ¯”è¼ƒ"""
    if not torch.cuda.is_available():
        print("âš ï¸ CUDA not available, skipping CPU vs GPU benchmark")
        return
    
    print("=" * 60)
    print("âš¡ CPU vs GPU æ€§èƒ½æ¯”è¼ƒ")
    print("=" * 60)
    
    test_texts = ["ãƒ†ã‚¹ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆ"] * 100
    test_images = [Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)) for _ in range(50)]
    
    # CPU ãƒ†ã‚¹ãƒˆ
    print("ğŸ–¥ï¸ CPUæ€§èƒ½æ¸¬å®š...")
    cpu_encoder = ColVBERTEncoder(device="cpu")
    
    start_time = time.time()
    cpu_embeddings = cpu_encoder.encode_text(test_texts[:20])  # å°‘æ•°ã§æ¸¬å®š
    cpu_time = time.time() - start_time
    
    # GPU ãƒ†ã‚¹ãƒˆ
    print("ğŸš€ GPUæ€§èƒ½æ¸¬å®š...")
    gpu_encoder = ColVBERTEncoder(device="cuda")
    
    start_time = time.time()
    gpu_embeddings = gpu_encoder.encode_text(test_texts[:20])
    gpu_time = time.time() - start_time
    
    print(f"\nğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ:")
    print(f"  CPUå‡¦ç†æ™‚é–“: {cpu_time:.2f}ç§’")
    print(f"  GPUå‡¦ç†æ™‚é–“: {gpu_time:.2f}ç§’")
    print(f"  é€Ÿåº¦å‘ä¸Š: {cpu_time/gpu_time:.1f}x")
    print(f"  GPUåŠ¹ç‡: {((cpu_time - gpu_time)/cpu_time)*100:.1f}% faster")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¥ Visual RAPTOR ColBERT GPU Test Suite")
    print("=" * 60)
    
    test_gpu_availability()
    
    if torch.cuda.is_available():
        test_colbert_gpu()
        benchmark_cpu_vs_gpu()
    else:
        print("âŒ CUDA not available. Please install CUDA-compatible PyTorch.")
        print("   Run: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")

if __name__ == "__main__":
    main()