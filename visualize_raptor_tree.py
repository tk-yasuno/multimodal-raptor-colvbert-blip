"""
RAPTOR Treeå¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
NetworkXã¨Matplotlibã‚’ä½¿ç”¨ã—ã¦ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’å¯è¦–åŒ–

ä¿å­˜ã•ã‚ŒãŸscaling_test_tree_*.pklãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰RAPTOR Treeã‚’èª­ã¿è¾¼ã¿ã€
éšå±¤æ§‹é€ ã‚’è¦–è¦šåŒ–ã—ã¦PNG/PDFã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚
"""

import os
import sys
import pickle
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib
from pathlib import Path
from typing import Dict, List, Tuple, Any
import numpy as np
from datetime import datetime
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer

# å½¢æ…‹ç´ è§£æ
try:
    from fugashi import Tagger, GenericTagger
    FUGASHI_AVAILABLE = True
except ImportError:
    FUGASHI_AVAILABLE = False
    print("âš ï¸ fugashiãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å½¢æ…‹ç´ è§£æãƒ©ãƒ™ãƒ«ã¯TF-IDFã®ã¿ä½¿ç”¨ã•ã‚Œã¾ã™")

# ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™
from disaster_vocab import filter_keywords, is_disaster_keyword, DISASTER_DOMAIN_KEYWORDS

# æ—¥æœ¬èªâ†’è‹±èªç¿»è¨³è¾æ›¸ï¼ˆç½å®³ç”¨èªï¼‰
# æ—¥æœ¬èªâ†’è‹±èªç¿»è¨³è¾æ›¸ï¼ˆç½å®³ç”¨èªï¼‰
JA_TO_EN_DICT = {
    # ç½å®³ç¨®åˆ¥
    'ç½å®³': 'Disaster', 'æ´¥æ³¢': 'Tsunami', 'åœ°éœ‡': 'Earthquake', 'å°é¢¨': 'Typhoon', 'è±ªé›¨': 'Heavy Rain',
    'æ´ªæ°´': 'Flood', 'åœŸç ‚': 'Landslide', 'ç«å±±': 'Volcano', 'å™´ç«': 'Eruption',
    'åœ°æ»‘ã‚Š': 'Landslide', 'å´©å£Š': 'Collapse', 'æµ¸æ°´': 'Inundation', 'æ°¾æ¿«': 'Overflow',
    'é«˜æ½®': 'Storm Surge', 'ç«œå·»': 'Tornado', 'åœŸçŸ³æµ': 'Debris Flow', 'å¤§éœ‡ç½': 'Great Earthquake',
    'éœ‡ç½': 'Earthquake Disaster',
    
    # é¿é›£ãƒ»å¯¾å¿œ
    'é¿é›£': 'Evacuation', 'é¿é›£æ‰€': 'Shelter', 'é¿é›£å ´æ‰€': 'Evac.Site', 'é¿é›£è€…': 'Evacuee',
    'é¿é›£æŒ‡ç¤º': 'Evac.Order', 'é¿é›£å‹§å‘Š': 'Evac.Advisory', 'é¿é›£çµŒè·¯': 'Evac.Route',
    'é¿é›£è¨“ç·´': 'Drill', 'é¿é›£è¡Œå‹•': 'Evac.Action', 'é¿é›£è¨ˆç”»': 'Evac.Plan', 'é¿é›£èª˜å°': 'Evac.Guide',
    'æ•‘åŠ©': 'Rescue', 'æ•‘æ´': 'Relief', 'æ•‘è­·': 'Aid', 'æ•‘å‡º': 'Rescue', 'æœç´¢': 'Search',
    'æ•‘å‘½': 'Lifesaving', 'æ•‘æ€¥': 'Emergency', 'å¿œæ´': 'Support',
    
    # è­¦å ±ãƒ»æƒ…å ±
    'è­¦å ±': 'Warning', 'æ³¨æ„å ±': 'Advisory', 'ç·Šæ€¥': 'Emergency', 'é€Ÿå ±': 'Alert',
    'æƒ…å ±': 'Info', 'ã‚¢ãƒ©ãƒ¼ãƒˆ': 'Alert', 'ç™ºä»¤': 'Issue', 'è§£é™¤': 'Lift',
    'ä¼é”': 'Transmission', 'é€šçŸ¥': 'Notification', 'å‘¨çŸ¥': 'Awareness', 'åºƒå ±': 'PR',
    'å ±å‘Š': 'Report', 'é€£çµ¡': 'Contact', 'é€šä¿¡': 'Communication',
    
    # çµ„ç¹”ãƒ»æ©Ÿé–¢
    'è‡ªæ²»ä½“': 'Municipality', 'å¸‚ç”ºæ‘': 'Municipality', 'éƒ½é“åºœçœŒ': 'Prefecture',
    'å›½': 'Nation', 'æ”¿åºœ': 'Government', 'æ°—è±¡åº': 'JMA', 'æ¶ˆé˜²': 'Fire Dept',
    'è­¦å¯Ÿ': 'Police', 'è‡ªè¡›éšŠ': 'SDF', 'æµ·ä¸Šä¿å®‰åº': 'Coast Guard', 'é˜²ç½': 'Disaster Prev',
    'æœ¬éƒ¨': 'HQ', 'å¯¾ç­–æœ¬éƒ¨': 'Response HQ', 'ç½å®³å¯¾ç­–': 'Disaster Response',
    'å±æ©Ÿç®¡ç†': 'Crisis Mgmt', 'è¡Œæ”¿': 'Administration', 'å¸‚': 'City', 'ç”º': 'Town',
    'æ‘': 'Village', 'çœŒ': 'Prefecture', 'åŒº': 'Ward',
    
    # è¢«å®³ãƒ»çŠ¶æ³
    'è¢«å®³': 'Damage', 'è¢«ç½': 'Disaster', 'è¢«ç½è€…': 'Victim', 'è¢«ç½åœ°': 'Affected Area',
    'æ­»è€…': 'Fatality', 'è¡Œæ–¹ä¸æ˜': 'Missing', 'è² å‚·': 'Injury', 'å€’å£Š': 'Collapse',
    'æå£Š': 'Destruction', 'å† æ°´': 'Flooding', 'å­¤ç«‹': 'Isolation',
    'åœé›»': 'Blackout', 'æ–­æ°´': 'Water Outage', 'é“è·¯': 'Road', 'é€šè¡Œæ­¢ã‚': 'Road Closed',
    'å¯¸æ–­': 'Cutoff', 'å…¨å£Š': 'Total Collapse', 'åŠå£Š': 'Partial Collapse',
    'æ¶²çŠ¶': 'Liquefaction',
    'æµ¸æ°´': 'Flooding', 'æ°´å®³': 'Flood Damage', 'é¢¨å®³': 'Wind Damage',
    
    # æ–½è¨­
    'å­¦æ ¡': 'School', 'ç—…é™¢': 'Hospital', 'å…¬æ°‘é¤¨': 'Community Ctr', 'ä½“è‚²é¤¨': 'Gym',
    'ãƒ›ãƒ¼ãƒ«': 'Hall', 'æ–½è¨­': 'Facility', 'æ¸¯': 'Port', 'æ¼æ¸¯': 'Fishing Port',
    'å ¤é˜²': 'Levee', 'ãƒ€ãƒ ': 'Dam', 'æ²³å·': 'River', 'æµ·å²¸': 'Coast',
    'å»ºç‰©': 'Building', 'ä½å®…': 'Housing', 'ä½å±…': 'Residence',
    
    # ä½æ°‘ãƒ»åœ°åŸŸ
    'ä½æ°‘': 'Resident', 'å¸‚æ°‘': 'Citizen', 'ç”ºæ°‘': 'Townspeople', 'æ‘æ°‘': 'Villager',
    'åœ°åŸŸ': 'Area', 'åœ°åŒº': 'District', 'ç”ºå†…': 'Neighborhood', 'ä¸–å¸¯': 'Household',
    'å®¶æ—': 'Family', 'å­ä¾›': 'Children', 'å­ã©ã‚‚': 'Children', 'å…ç«¥': 'Children',
    'é«˜é½¢è€…': 'Elderly', 'è¦é…æ…®è€…': 'Vulnerable', 'ä¹³å¹¼å…': 'Infant', 'åœ’å…': 'Preschooler',
    'ä½å®…åœ°': 'Residential', 'å•†æ¥­åœ°': 'Commercial', 'äººå£': 'Population',
    
    # å¯¾ç­–ãƒ»è¨ˆç”»
    'å¯¾ç­–': 'Measure', 'è¨ˆç”»': 'Plan', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«': 'Manual', 'ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³': 'Guideline',
    'è¨“ç·´': 'Training', 'æƒ³å®š': 'Scenario', 'ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³': 'Simulation',
    'ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—': 'Hazard Map', 'é˜²ç½è¨ˆç”»': 'Disaster Plan', 'ä½“åˆ¶': 'System',
    'æ•´å‚™': 'Development', 'æº–å‚™': 'Preparation', 'å¼·åŒ–': 'Reinforcement',
    'æ–½ç­–': 'Policy', 'æ–¹é‡': 'Guideline', 'æˆ¦ç•¥': 'Strategy', 'å–çµ„': 'Initiative',
    'å‚™ãˆ': 'Preparedness',
    
    # æ™‚é–“ãƒ»ãƒ•ã‚§ãƒ¼ã‚º
    'ç™ºç”Ÿ': 'Occurrence', 'ç›´å¾Œ': 'Immediate', 'å½“æ—¥': 'Same Day', 'ç¿Œæ—¥': 'Next Day',
    'æ•°æ—¥å¾Œ': 'Days Later', 'å¾©æ—§': 'Recovery', 'å¾©èˆˆ': 'Reconstruction',
    'æ—©æœŸ': 'Early', 'è¿…é€Ÿ': 'Prompt', 'å³åº§': 'Instant',
    
    # å›ºæœ‰åè©ï¼ˆåœ°åï¼‰
    'æ—¥æœ¬': 'Japan', 'æ±æ—¥æœ¬': 'East Japan', 'é˜ªç¥': 'Hanshin', 'ç†Šæœ¬': 'Kumamoto', 'åŒ—æµ·é“': 'Hokkaido',
    'ä¹å·': 'Kyushu', 'å››å›½': 'Shikoku', 'å¤ªå¹³æ´‹': 'Pacific', 'æ—¥æœ¬æµ·': 'Sea of Japan',
    'å—æµ·ãƒˆãƒ©ãƒ•': 'Nankai Trough', 'æ±æµ·': 'Tokai', 'é–¢æ±': 'Kanto', 'æ±åŒ—': 'Tohoku',
    'ä¸­éƒ¨': 'Chubu', 'è¿‘ç•¿': 'Kinki', 'ä¸­å›½': 'Chugoku', 'æ²–ç¸„': 'Okinawa', 'é–¢è¥¿': 'Kansai',
    'åŠå³¶': 'Peninsula', 'èƒ½ç™»åŠå³¶': 'Noto Peninsula', 'èƒ½ç™»': 'Noto', 'çŸ³å·': 'Ishikawa', 'ç¦å³¶': 'Fukushima',
    'å®®åŸ': 'Miyagi', 'å²©æ‰‹': 'Iwate', 'æ·¡è·¯': 'Awaji', 'æ±å³°': 'Toho', 'æ—¥ç”°': 'Hita',
    'ç™»åˆ¥': 'Noboribetsu', 'æœ­å¹Œ': 'Sapporo', 'ä»™å°': 'Sendai', 'ç¥æˆ¸': 'Kobe',
    'åºƒå³¶': 'Hiroshima', 'é•·å´': 'Nagasaki', 'é¹¿å…å³¶': 'Kagoshima', 'å¹³æˆ': 'Heisei',
    'æ²¿å²¸': 'Coastal', 'å¸‚ç”º': 'City-Town',
    
    # ä¸€èˆ¬çš„ãªå‹•è©ãƒ»å½¢å®¹è©
    'ç¢ºèª': 'Confirm', 'å®Ÿæ–½': 'Implement', 'è¨­ç½®': 'Install', 'é…ç½®': 'Deploy',
    'é–‹è¨­': 'Open', 'é‹å–¶': 'Operate', 'æä¾›': 'Provide', 'æ”¯æ´': 'Support',
    'å¿…è¦': 'Necessary', 'é‡è¦': 'Important', 'å®‰å…¨': 'Safety', 'å±é™º': 'Danger',
    'é©åˆ‡': 'Appropriate', 'æœ‰åŠ¹': 'Effective', 'å¯èƒ½': 'Possible',
    
    # ãã®ä»–ã®é‡è¦èª
    'æ˜ç¢º': 'Clear', 'è©³ç´°': 'Detail', 'å…·ä½“': 'Specific', 'å…¨ä½“': 'Overall',
    'ä¸€éƒ¨': 'Partial', 'å¤šæ•°': 'Many', 'å°‘æ•°': 'Few', 'å¤§è¦æ¨¡': 'Large-scale',
    'å°è¦æ¨¡': 'Small-scale', 'ä¸­è¦æ¨¡': 'Medium-scale', 'æ®‹å¿µ': 'Regrettable',
    'æ‰‹å…ƒ': 'On Hand',
    
    # æ•™è¨“ãƒ»çµŒé¨“
    'æ•™è¨“': 'Lesson', 'çµŒé¨“': 'Experience', 'äº‹ä¾‹': 'Case', 'è¨˜éŒ²': 'Record',
    'è³‡æ–™': 'Document', 'å‚è€ƒ': 'Reference', 'ãƒ‡ãƒ¼ã‚¿': 'Data',
    
    # ã‚«ã‚¿ã‚«ãƒŠèª
    'ãƒšãƒ¼ã‚¸': 'Page', 'ã‚±ãƒ¼ã‚¹': 'Case', 'ãƒ¬ãƒ™ãƒ«': 'Level', 'ã‚·ã‚¹ãƒ†ãƒ ': 'System',
    'ã‚»ãƒ³ã‚¿ãƒ¼': 'Center', 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯': 'Network', 'ã‚¨ãƒªã‚¢': 'Area', 'ã‚±ã‚¢': 'Care',
    'ãƒ†ã‚­ã‚¹ãƒˆ': 'Text',
    
    # è¿½åŠ ã®ä¸€èˆ¬èªå½™ï¼ˆç™½æŠœãé˜²æ­¢ï¼‰
    'æ˜ã‚‰ã‹': 'Clear', 'ç¢ºå®Ÿ': 'Certain', 'å¿…è¦': 'Necessary', 'é‡è¦': 'Important',
    'å¯èƒ½': 'Possible', 'å›°é›£': 'Difficult', 'ååˆ†': 'Sufficient', 'ä¸è¶³': 'Shortage',
    'å•é¡Œ': 'Problem', 'èª²é¡Œ': 'Issue', 'æ”¹å–„': 'Improvement', 'å¤‰æ›´': 'Change',
    'å¢—åŠ ': 'Increase', 'æ¸›å°‘': 'Decrease', 'ç¶­æŒ': 'Maintain', 'å¼·åŒ–': 'Strengthen',
    'æ•´å‚™': 'Preparation', 'è¨­ç½®': 'Installation', 'é…ç½®': 'Placement', 'ç§»å‹•': 'Movement',
    'é–‹å§‹': 'Start', 'çµ‚äº†': 'End', 'ç¶™ç¶š': 'Continue', 'ä¸­æ­¢': 'Suspend',
    'åˆ¤æ–­': 'Judgment', 'æ±ºå®š': 'Decision', 'é¸æŠ': 'Choice', 'æ¤œè¨': 'Consideration',
    'èª¿æŸ»': 'Survey', 'åˆ†æ': 'Analysis', 'è©•ä¾¡': 'Evaluation', 'ç¢ºèª': 'Confirmation',
    'é€£çµ¡': 'Contact', 'é€£æº': 'Cooperation', 'å”åŠ›': 'Collaboration', 'æ”¯æ´': 'Support',
    'ç®¡ç†': 'Management', 'é‹å–¶': 'Operation', 'æ´»å‹•': 'Activity', 'è¡Œå‹•': 'Action',
    'æŒ‡å°': 'Guidance', 'æ•™è‚²': 'Education', 'å­¦ç¿’': 'Learning', 'ç†è§£': 'Understanding',
    'èª¬æ˜': 'Explanation', 'æŒ‡ç¤º': 'Instruction', 'è¦è«‹': 'Request', 'ä¾é ¼': 'Request',
    'å ±é“': 'Report', 'å…¬è¡¨': 'Announcement', 'ç™ºè¡¨': 'Publication', 'é€šå ±': 'Notification',
    'å—ä¿¡': 'Reception', 'é€ä¿¡': 'Transmission', 'é…ä¿¡': 'Distribution', 'æä¾›': 'Provision',
    'åˆ©ç”¨': 'Use', 'ä½¿ç”¨': 'Usage', 'æ´»ç”¨': 'Utilization', 'é©ç”¨': 'Application',
    'å®Ÿè¡Œ': 'Execution', 'é‚è¡Œ': 'Performance', 'é”æˆ': 'Achievement', 'å®Œäº†': 'Completion',
    # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‹ã‚‰è¿½åŠ ï¼ˆé »å‡ºæœªç™»éŒ²èªï¼‰
    'å¿ƒèº«': 'Mind-Body', 'è¤‡æ•°': 'Multiple', 'å¯¾å¿œ': 'Response', 'æŠŠæ¡': 'Grasp',
    'æ±åŒ—å¤§': 'Tohoku Univ', 'æ–‡åŒ–è²¡': 'Cultural Property', 'å¥‘æ©Ÿ': 'Opportunity',
    'ä»®è¨­': 'Temporary', 'ä¸­å¿ƒ': 'Center', 'å„åœ°': 'Various Places', 'åŒ—éƒ¨': 'Northern Part',
    'æ··ä¹±': 'Confusion', 'ç¢ºä¿': 'Secure', 'ç•ªå·': 'Number', 'é…ã‚Œ': 'Delay',
    'åŸç™º': 'Nuclear Plant', 'æ··é›‘': 'Congestion', 'æ•°å€¤': 'Numerical Value',
}

# pykakasiã®ã‚°ãƒ­ãƒ¼ãƒãƒ«åˆæœŸåŒ–ï¼ˆ1å›ã®ã¿ï¼‰
_KAKASI_INSTANCE = None

def get_kakasi():
    """pykakasiã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _KAKASI_INSTANCE
    if _KAKASI_INSTANCE is None:
        try:
            import pykakasi
            _KAKASI_INSTANCE = pykakasi.kakasi()
        except ImportError:
            _KAKASI_INSTANCE = False  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—ã‚’ãƒãƒ¼ã‚¯
    return _KAKASI_INSTANCE

def translate_keyword(keyword: str) -> str:
    """
    æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‹±èªã«ç¿»è¨³ï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹ + ãƒ­ãƒ¼ãƒå­—åŒ–ï¼‰
    
    1. æ—¢ã«ASCIIãªã‚‰ãã®ã¾ã¾è¿”ã™
    2. ç¿»è¨³è¾æ›¸ã‹ã‚‰æ¤œç´¢
    3. pykakasiã§ãƒ­ãƒ¼ãƒå­—åŒ–
    4. å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    # æ—¢ã«è‹±æ•°å­—ã®ã¿ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if keyword.isascii():
        return keyword
    
    # è¾æ›¸ã‹ã‚‰ç¿»è¨³ã‚’å–å¾—
    translated = JA_TO_EN_DICT.get(keyword)
    if translated:
        return translated
    
    # pykakasiã§ãƒ­ãƒ¼ãƒå­—åŒ–ã‚’è©¦è¡Œ
    kks = get_kakasi()
    if kks and kks is not False:
        try:
            result = kks.convert(keyword)
            # hepburnã‚­ãƒ¼ã§ãƒ­ãƒ¼ãƒå­—ã‚’å–å¾—ã—ã€å˜èªã”ã¨ã«å…ˆé ­å¤§æ–‡å­—åŒ–
            romaji_parts = []
            for item in result:
                if 'hepburn' in item and item['hepburn']:
                    romaji_parts.append(item['hepburn'].capitalize())
            
            if romaji_parts:
                return ''.join(romaji_parts)
                
        except Exception as e:
            # ãƒ‡ãƒãƒƒã‚°: ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’å‡ºåŠ›
            print(f"      âš ï¸ ãƒ­ãƒ¼ãƒå­—åŒ–ã‚¨ãƒ©ãƒ¼ '{keyword}': {e}")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ASCIIã®ã¿ã®ä»£æ›¿è¡¨ç¾ã‚’è¿”ã™
    import re
    # æ•°å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ•°å­—ã‚’æŠ½å‡º
    numbers = re.findall(r'\d+', keyword)
    if numbers:
        return f"Item{numbers[0]}"
    # ãã‚Œä»¥å¤–ã¯æ±ç”¨çš„ãªè¡¨ç¾ï¼ˆãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼‰
    print(f"      âš ï¸ ç¿»è¨³å¤±æ•— '{keyword}' â†’ 'Term'")
    return "Term"

# UTF-8å‡ºåŠ›è¨­å®šï¼ˆWindowså¯¾å¿œï¼‰
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
matplotlib.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '0_base_tsunami-lesson-rag'))


class RAPTORTreeVisualizer:
    """RAPTOR Treeã®å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, tree_path: str, use_morphology: bool = True, top_n: int = 2):
        """
        Args:
            tree_path: RAPTORãƒ„ãƒªãƒ¼ã®pickleãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            use_morphology: å½¢æ…‹ç´ è§£æã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆTrue: MeCabå½¢æ…‹ç´ è§£æã€False: TF-IDFã®ã¿ï¼‰
            top_n: ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ã«è¡¨ç¤ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰
        """
        self.tree_path = Path(tree_path)
        self.tree = None
        self.graph = None
        self.node_depths = {}
        self.node_types = {}  # 'leaf' or 'internal'
        self.node_keywords = {}  # ãƒãƒ¼ãƒ‰ã”ã¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.all_summaries = []  # TF-IDFè¨ˆç®—ç”¨ã®å…¨ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
        self.node_to_summary_idx = {}  # ãƒãƒ¼ãƒ‰IDã‹ã‚‰ã‚µãƒãƒªãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.node_parents = {}  # ãƒãƒ¼ãƒ‰IDã‹ã‚‰è¦ªãƒãƒ¼ãƒ‰IDã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆéšå±¤çš„é™¤å¤–ç”¨ï¼‰
        self.used_keywords_by_depth = {}  # æ·±åº¦ã”ã¨ã«ä½¿ç”¨æ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨˜éŒ²
        self.top_n = top_n  # ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°
        
        # å½¢æ…‹ç´ è§£æè¨­å®š
        self.use_morphology = use_morphology and FUGASHI_AVAILABLE
        if self.use_morphology:
            try:
                import os
                
                # MeCabã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
                mecabrc_paths = [
                    r'C:\Program Files\MeCab\etc\mecabrc',
                    r'C:\Program Files (x86)\MeCab\etc\mecabrc',
                    r'C:\mecab\etc\mecabrc'
                ]
                
                for mecabrc_path in mecabrc_paths:
                    if os.path.exists(mecabrc_path):
                        os.environ['MECABRC'] = mecabrc_path
                        print(f"   âš™ï¸ MECABRC: {mecabrc_path}")
                        break
                
                # MeCabã®è¾æ›¸ãƒ‘ã‚¹è¨­å®š
                # å„ªå…ˆé †ä½: 1) unidic-lite, 2) ã‚·ã‚¹ãƒ†ãƒ MeCab, 3) ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                dicdir = None
                
                # 1. unidic-liteã‚’è©¦è¡Œ
                try:
                    import unidic_lite
                    dicdir = unidic_lite.DICDIR
                    print(f"   ğŸ“– è¾æ›¸: unidic-lite ({dicdir})")
                except ImportError:
                    pass
                
                # 2. ã‚·ã‚¹ãƒ†ãƒ MeCabã‚’è©¦è¡Œï¼ˆC:\Program Files\MeCabï¼‰
                if dicdir is None:
                    mecab_dic_paths = [
                        r'C:\Program Files\MeCab\dic\ipadic',
                        r'C:\Program Files (x86)\MeCab\dic\ipadic',
                        r'C:\mecab\dic\ipadic'
                    ]
                    for path in mecab_dic_paths:
                        if os.path.exists(path):
                            dicdir = path
                            print(f"   ğŸ“– è¾æ›¸: ã‚·ã‚¹ãƒ†ãƒ MeCab ({dicdir})")
                            break
                
                # TaggeråˆæœŸåŒ–ï¼ˆGenericTagger for IPADICï¼‰
                if dicdir:
                    # IPADICå½¢å¼ã®è¾æ›¸ã«ã¯GenericTaggerã‚’ä½¿ç”¨
                    if 'ipadic' in dicdir.lower():
                        self.tagger = GenericTagger(f'-d "{dicdir}"')
                        print("   ğŸ”§ Tagger: GenericTagger (IPADICå¯¾å¿œ)")
                    else:
                        # UniDicå½¢å¼
                        self.tagger = Tagger(f'-d "{dicdir}"')
                        print("   ğŸ”§ Tagger: Tagger (UniDic)")
                else:
                    # è¾æ›¸ãƒ‘ã‚¹æŒ‡å®šãªã—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
                    try:
                        self.tagger = Tagger()
                        print("   ğŸ“– è¾æ›¸: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (Tagger)")
                    except:
                        self.tagger = GenericTagger()
                        print("   ğŸ“– è¾æ›¸: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (GenericTagger)")
                
                print("âœ… å½¢æ…‹ç´ è§£æãƒ¢ãƒ¼ãƒ‰: MeCab (fugashi) + ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ãƒ•ã‚£ãƒ«ã‚¿")
            except Exception as e:
                print(f"âš ï¸ å½¢æ…‹ç´ è§£æåˆæœŸåŒ–å¤±æ•—: {e}")
                print("   TF-IDFãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                self.use_morphology = False
                self.tagger = None
        else:
            self.tagger = None
            if use_morphology and not FUGASHI_AVAILABLE:
                print("âš ï¸ fugashiæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚TF-IDFãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
            else:
                print("ğŸ“Š TF-IDFãƒ¢ãƒ¼ãƒ‰")
    
    def extract_keywords_morphology(self, text: str, top_n: int = 10) -> List[str]:
        """
        å½¢æ…‹ç´ è§£æã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ï¼‰
        
        Args:
            text: è§£æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            top_n: æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆå€™è£œã‚’å¤šã‚ã«å–å¾—ï¼‰
        
        Returns:
            ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆä½“è¨€æ­¢ã‚ã€ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™å„ªå…ˆï¼‰
        """
        if not self.use_morphology or not text:
            return []
        
        # å½¢æ…‹ç´ è§£æå®Ÿè¡Œ
        words = []
        for word in self.tagger(text):
            # å“è©å–å¾—ï¼ˆGenericTaggerã¨Taggerã®ä¸¡å¯¾å¿œï¼‰
            if hasattr(word, 'feature'):
                # Tagger (UniDic)
                pos = word.feature.pos1 if hasattr(word.feature, 'pos1') else word.feature[0]
            else:
                # GenericTagger (IPADIC) - featuresã¯æ–‡å­—åˆ—ã®ã‚¿ãƒ—ãƒ«
                features = word.features if hasattr(word, 'features') else str(word).split('\t')[1].split(',')
                pos = features[0] if features else ''
            
            # åè©ãƒ»å›ºæœ‰åè©ã®ã¿æŠ½å‡º
            if pos in ['åè©', 'å›ºæœ‰åè©']:
                surface = word.surface if hasattr(word, 'surface') else str(word).split('\t')[0]
                # 1æ–‡å­—ã¯é™¤å¤–ï¼ˆåŠ©è©çš„ãªåè©ã‚’æ’é™¤ï¼‰
                if len(surface) >= 2:
                    words.append(surface)
        
        # ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‹å„ªå…ˆé †ä½ä»˜ã‘
        # æˆ¦ç•¥: ä¸Šä½å€™è£œã‚’å¤šã‚ã«å–å¾—ã—ã¦ã€å¾Œã§éšå±¤çš„é™¤å¤–ã‚’é©ç”¨
        from disaster_vocab import DISASTER_DOMAIN_KEYWORDS
        
        # é »å‡ºã™ã‚‹è¶…ä¸€èˆ¬çš„ãªç½å®³èªå½™ï¼ˆå„ãƒãƒ¼ãƒ‰å…±é€šã«ãªã‚Šã‚„ã™ã„ï¼‰
        common_disaster_words = {'æ´¥æ³¢', 'é¿é›£', 'åœ°éœ‡', 'è¢«å®³', 'ä½æ°‘', 'å¯¾ç­–', 'ç™ºç”Ÿ', 'æƒ…å ±'}
        
        filtered = filter_keywords(words, prioritize_domain=True)
        
        # æˆ¦ç•¥: ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ã®ä¸­ã§ã‚‚å…±é€šèªã‚’é¿ã‘ã€ç‰¹å¾´çš„ãªèªã‚’å„ªå…ˆ
        specific_keywords = []
        common_keywords = []
        
        # ã€æ”¹å–„ã€‘å€™è£œæ•°ã‚’50èªã«æ‹¡å¤§ï¼ˆéšå±¤çš„é™¤å¤–ã«ã‚ˆã‚Šå¤šããŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ï¼‰
        for word in filtered[:50]:
            if word in common_disaster_words:
                common_keywords.append(word)
            else:
                specific_keywords.append(word)
        
        # ç‰¹å¾´çš„ãªèªã‚’å„ªå…ˆã€è¶³ã‚Šãªã‘ã‚Œã°å…±é€šèªã§è£œå®Œ
        result = specific_keywords + common_keywords
        
        return result[:top_n]
    
    def extract_keywords_tfidf(self, texts: List[str], top_n: int = 10) -> List[List[str]]:
        """
        TF-IDFã‚’ä½¿ç”¨ã—ã¦å„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ã‚’å„ªå…ˆã—ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
        å…±é€šèªã‚’é¿ã‘ã¦ç‰¹å¾´çš„ãªèªã‚’é¸æŠ
        
        Args:
            texts: ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            top_n: å„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆå€™è£œã‚’å¤šã‚ã«å–å¾—ï¼‰
        
        Returns:
            å„ãƒ†ã‚­ã‚¹ãƒˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        """
        if not texts or len(texts) == 0:
            return []
        
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼è¨­å®š
        # token_pattern: æ—¥æœ¬èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰ã¨è‹±èªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰ã‚’ãƒãƒƒãƒ
        vectorizer = TfidfVectorizer(
            token_pattern=r'[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]{2,}|[a-zA-Z]{3,}',
            max_features=1000,
            stop_words=None
        )
        
        # ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        from disaster_vocab import STOP_WORDS, is_disaster_keyword
        
        # é »å‡ºã™ã‚‹è¶…ä¸€èˆ¬çš„ãªç½å®³èªå½™ï¼ˆå„ãƒãƒ¼ãƒ‰å…±é€šã«ãªã‚Šã‚„ã™ã„ï¼‰
        common_disaster_words = {'æ´¥æ³¢', 'é¿é›£', 'åœ°éœ‡', 'è¢«å®³', 'ä½æ°‘', 'å¯¾ç­–', 'ç™ºç”Ÿ', 'æƒ…å ±'}
        
        try:
            # TF-IDFè¡Œåˆ—ã‚’è¨ˆç®—
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # å„ãƒ†ã‚­ã‚¹ãƒˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            all_keywords = []
            for i in range(len(texts)):
                # iç•ªç›®ã®ãƒ†ã‚­ã‚¹ãƒˆã®TF-IDFã‚¹ã‚³ã‚¢ã‚’å–å¾—
                tfidf_scores = tfidf_matrix[i].toarray()[0]
                
                # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
                top_indices = tfidf_scores.argsort()[::-1]
                
                # ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ã¨ä¸€èˆ¬èªã‚’åˆ†é¡
                specific_keywords = []  # ç‰¹å¾´çš„ãªèª
                common_keywords = []    # å…±é€šã®ç½å®³èª
                other_keywords = []     # ãã®ä»–
                
                # ã€æ”¹å–„ã€‘å€™è£œæ•°ã‚’50èªã«æ‹¡å¤§ï¼ˆéšå±¤çš„é™¤å¤–ã«ã‚ˆã‚Šå¤šããŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ï¼‰
                for idx in top_indices[:50]:
                    word = feature_names[idx]
                    if tfidf_scores[idx] <= 0:
                        continue
                    
                    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
                    if word in STOP_WORDS:
                        continue
                    
                    # åˆ†é¡
                    if word in common_disaster_words:
                        common_keywords.append(word)
                    elif is_disaster_keyword(word):
                        specific_keywords.append(word)
                    else:
                        other_keywords.append(word)
                
                # å„ªå…ˆé †ä½: ç‰¹å¾´çš„ç½å®³èª > ãã®ä»– > å…±é€šç½å®³èª
                keywords = (specific_keywords + other_keywords + common_keywords)[:top_n]
                all_keywords.append(keywords)
            
            return all_keywords
            
        except Exception as e:
            print(f"âš ï¸ TF-IDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
            return [[] for _ in texts]
        
    def load_tree(self):
        """Pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ„ãƒªãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“‚ ãƒ„ãƒªãƒ¼èª­ã¿è¾¼ã¿ä¸­: {self.tree_path.name}")
        
        with open(self.tree_path, 'rb') as f:
            data = pickle.load(f)
        
        # è¾æ›¸å½¢å¼ã®å ´åˆã€'tree'ã‚­ãƒ¼ã‹ã‚‰ãƒ„ãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’å–å¾—
        if isinstance(data, dict) and 'tree' in data:
            self.tree = data['tree']
            print(f"âœ… ãƒ„ãƒªãƒ¼èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆè¾æ›¸å½¢å¼ï¼‰")
        else:
            self.tree = data
            print(f"âœ… ãƒ„ãƒªãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
        
        return self.tree
    
    def build_graph(self):
        """NetworkXã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
        print("ğŸ”¨ NetworkXã‚°ãƒ©ãƒ•æ§‹ç¯‰ä¸­...")
        
        self.graph = nx.DiGraph()
        
        # ãƒãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.node_counter = 0
        self.leaf_nodes = []
        self.internal_nodes = []
        
        # ç¬¬1ãƒ‘ã‚¹: å…¨ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†
        temp_nodes = []  # (node_id, cluster_id, summary_text, depth, parent_id, has_children) ã®ãƒªã‚¹ãƒˆ
        
        def get_node_id():
            """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ¼ãƒ‰IDã‚’ç”Ÿæˆ"""
            self.node_counter += 1
            return f"node_{self.node_counter}"
        
        def collect_summaries(tree_dict, depth=0, parent_id=None):
            """
            ç¬¬1ãƒ‘ã‚¹: ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†
            """
            if not isinstance(tree_dict, dict):
                return
            
            if 'clusters' in tree_dict and tree_dict['clusters']:
                clusters = tree_dict['clusters']
                
                for cluster_id, cluster_data in clusters.items():
                    # ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    summary_text = ""
                    if 'summary' in cluster_data:
                        summary = cluster_data['summary']
                        if hasattr(summary, 'page_content'):
                            summary_text = summary.page_content
                        elif isinstance(summary, dict):
                            summary_text = summary.get('page_content', summary.get('text', ''))
                    
                    # å­ã®æœ‰ç„¡åˆ¤å®š
                    has_children = ('children' in cluster_data and 
                                  cluster_data['children'] and 
                                  'clusters' in cluster_data['children'] and
                                  cluster_data['children']['clusters'])
                    
                    # ãƒãƒ¼ãƒ‰IDç”Ÿæˆ
                    node_id = get_node_id()
                    
                    # æƒ…å ±ã‚’ä¿å­˜
                    temp_nodes.append({
                        'node_id': node_id,
                        'cluster_id': cluster_id,
                        'summary_text': summary_text,
                        'depth': depth,
                        'parent_id': parent_id,
                        'has_children': has_children
                    })
                    
                    # ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    self.all_summaries.append(summary_text if summary_text else "")
                    self.node_to_summary_idx[node_id] = len(self.all_summaries) - 1
                    
                    # å­ãƒ„ãƒªãƒ¼ã‚’å†å¸°çš„ã«å‡¦ç†
                    if has_children:
                        collect_summaries(
                            cluster_data['children'],
                            depth + 1,
                            node_id
                        )
        
        # ç¬¬1ãƒ‘ã‚¹å®Ÿè¡Œ: ã‚µãƒãƒªãƒ¼åé›†
        if isinstance(self.tree, dict):
            collect_summaries(self.tree)
        else:
            print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ„ãƒªãƒ¼ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            return None
        
        print(f"   ç¬¬1ãƒ‘ã‚¹å®Œäº†: {len(self.all_summaries)}å€‹ã®ã‚µãƒãƒªãƒ¼ã‚’åé›†")
        
        # è¦ªå­é–¢ä¿‚ãƒãƒƒãƒ—æ§‹ç¯‰
        for node_info in temp_nodes:
            node_id = node_info['node_id']
            parent_id = node_info['parent_id']
            if parent_id:
                self.node_parents[node_id] = parent_id
        
        # ç¬¬2ãƒ‘ã‚¹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå½¢æ…‹ç´ è§£æ or TF-IDFï¼‰
        if self.use_morphology:
            # å½¢æ…‹ç´ è§£æãƒ¢ãƒ¼ãƒ‰
            print("   ğŸ§  å½¢æ…‹ç´ è§£æã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­ï¼ˆç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ– + éšå±¤çš„é™¤å¤–ï¼‰...")
            all_keywords = []
            for summary_text in self.all_summaries:
                # ã€æ”¹å–„ã€‘ä¸Šä½20èªã®å€™è£œã‚’å–å¾—ï¼ˆå¾Œã§éšå±¤çš„é™¤å¤–ã‚’é©ç”¨ï¼‰
                keywords = self.extract_keywords_morphology(summary_text, top_n=20)
                all_keywords.append(keywords)
        else:
            # TF-IDFãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ï¼‰
            print("   ğŸ“Š TF-IDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­ï¼ˆéšå±¤çš„é™¤å¤–ï¼‰...")
            if len(self.all_summaries) > 0:
                # ã€æ”¹å–„ã€‘ä¸Šä½20èªã®å€™è£œã‚’å–å¾—ï¼ˆå¾Œã§éšå±¤çš„é™¤å¤–ã‚’é©ç”¨ï¼‰
                all_keywords = self.extract_keywords_tfidf(self.all_summaries, top_n=20)
            else:
                all_keywords = []
        
        # ç¬¬3ãƒ‘ã‚¹: ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆæ·±åº¦é †ã«ã‚½ãƒ¼ãƒˆã—ã¦å‡¦ç†ï¼‰
        print("   ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ä½œæˆä¸­ï¼ˆéšå±¤çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é™¤å¤–é©ç”¨ï¼‰...")
        
        # æ·±åº¦é †ã«ã‚½ãƒ¼ãƒˆï¼ˆä¸Šä½ã‹ã‚‰ä¸‹ä½ã¸ï¼‰
        temp_nodes_sorted = sorted(temp_nodes, key=lambda x: x['depth'])
        
        for node_info in temp_nodes_sorted:
            node_id = node_info['node_id']
            cluster_id = node_info['cluster_id']
            summary_text = node_info['summary_text']
            depth = node_info['depth']
            parent_id = node_info['parent_id']
            has_children = node_info['has_children']
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
            summary_idx = self.node_to_summary_idx[node_id]
            raw_keywords = all_keywords[summary_idx] if summary_idx < len(all_keywords) else []
            
            # ã€æ”¹å–„ã€‘éšå±¤çš„é™¤å¤–: ã‚ˆã‚Šå³å¯†ãªç¥–å…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†
            ancestor_keywords = set()
            
            # 1. è¦ªã®ç³»è­œã‚’è¾¿ã£ã¦ã€å…¨ã¦ã®ç¥–å…ˆãƒãƒ¼ãƒ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†
            current_parent = parent_id
            while current_parent:
                if current_parent in self.node_keywords:
                    ancestor_keywords.update(self.node_keywords[current_parent])
                current_parent = self.node_parents.get(current_parent)
            
            # 2. ã‚ˆã‚Šæµ…ã„å…¨ã¦ã®æ·±åº¦ã§ä½¿ç”¨ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚é™¤å¤–
            # ä¾‹: depth=2ã®ãƒãƒ¼ãƒ‰ã¯ã€depth=0ã¨depth=1ã®å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
            for d in range(depth):
                if d in self.used_keywords_by_depth:
                    ancestor_keywords.update(self.used_keywords_by_depth[d])
            
            # 3. ç¾åœ¨ã®æ·±åº¦ã§æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚é™¤å¤–ï¼ˆå…„å¼Ÿãƒãƒ¼ãƒ‰ï¼‰
            if depth in self.used_keywords_by_depth:
                ancestor_keywords.update(self.used_keywords_by_depth[depth])
            
            # ã€æ”¹å–„ã€‘ç¥–å…ˆã¨é‡è¤‡ã—ãªã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿é¸æŠ
            # ã¾ãšã€å®Œå…¨ã«é™¤å¤–ã•ã‚Œã‚‹ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_keywords = []
            remaining_keywords = []
            
            for kw in raw_keywords:
                if kw not in ancestor_keywords:
                    remaining_keywords.append(kw)
            
            # ã€æ”¹å–„2ã€‘åŒä¸€ãƒãƒ¼ãƒ‰å†…ã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¤‡ã‚’é˜²ã
            # remaining_keywordsã‹ã‚‰é‡è¤‡ãªã—ã§top_nå€‹é¸æŠ
            seen_in_node = set()
            for kw in remaining_keywords:
                if kw not in seen_in_node:
                    filtered_keywords.append(kw)
                    seen_in_node.add(kw)
                    if len(filtered_keywords) >= self.top_n:
                        break
            
            # å€™è£œãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®è­¦å‘Š
            if len(filtered_keywords) < self.top_n:
                print(f"      âš ï¸ ãƒãƒ¼ãƒ‰{node_id} (depth={depth}): ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸è¶³ ({len(filtered_keywords)}/{self.top_n})")
            
            keywords = filtered_keywords[:self.top_n] if filtered_keywords else []
            self.node_keywords[node_id] = keywords
            
            # ä½¿ç”¨æ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨˜éŒ²
            if depth not in self.used_keywords_by_depth:
                self.used_keywords_by_depth[depth] = set()
            self.used_keywords_by_depth[depth].update(keywords)
            
            # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¤å®š
            node_type = 'internal' if has_children else 'leaf'
            
            if node_type == 'leaf':
                self.leaf_nodes.append(node_id)
            else:
                self.internal_nodes.append(node_id)
            
            self.node_types[node_id] = node_type
            self.node_depths[node_id] = depth
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ä½œæˆï¼ˆä½“è¨€æ­¢ã‚ã€ç°¡æ½”ã«ï¼‰
            if keywords:
                if self.use_morphology:
                    # å½¢æ…‹ç´ è§£æ: ä½“è¨€æ­¢ã‚ã‚’ã€Œãƒ»ã€ã§æ¥ç¶š
                    # ä¾‹: "é¿é›£ãƒ»è­¦å ±ãƒ»è¨“ç·´" (top_n=3ã®å ´åˆ)
                    keyword_label = "ãƒ»".join(keywords[:self.top_n])
                else:
                    # TF-IDF: ä½“è¨€æ­¢ã‚ã‚’ã€Œãƒ»ã€ã§æ¥ç¶šï¼ˆç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³å„ªå…ˆï¼‰
                    keyword_label = "ãƒ»".join(keywords[:self.top_n])
            else:
                keyword_label = f"C{cluster_id}"
            
            self.graph.add_node(
                node_id,
                depth=depth,
                text=str(summary_text[:100]) if summary_text else "",
                keywords=keywords,
                node_type=node_type,
                label=keyword_label
            )
            
            # è¦ªãƒãƒ¼ãƒ‰ã¨ã®æ¥ç¶š
            if parent_id:
                self.graph.add_edge(parent_id, node_id)
        
        if self.graph.number_of_nodes() == 0:
            print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None
        
        print(f"âœ… ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†")
        print(f"   ç·ãƒãƒ¼ãƒ‰æ•°: {self.graph.number_of_nodes()}")
        print(f"   ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰: {len(self.leaf_nodes)}")
        print(f"   å†…éƒ¨ãƒãƒ¼ãƒ‰: {len(self.internal_nodes)}")
        print(f"   ã‚¨ãƒƒã‚¸æ•°: {self.graph.number_of_edges()}")
        print(f"   æœ€å¤§æ·±åº¦: {max(self.node_depths.values()) if self.node_depths else 0}")
        
        return self.graph
    
    def visualize_tree(self, output_path: str, figsize: Tuple[int, int] = None, dpi: int = 150):
        """
        ãƒ„ãƒªãƒ¼ã‚’å¯è¦–åŒ–ã—ã¦ä¿å­˜
        
        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆPNG/PDFï¼‰
            figsize: å›³ã®ã‚µã‚¤ã‚ºï¼ˆå¹…ã€é«˜ã•ï¼‰ã€‚Noneã®å ´åˆã¯ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦è‡ªå‹•èª¿æ•´
            dpi: è§£åƒåº¦
        """
        if self.graph is None:
            print("âŒ ã‚¨ãƒ©ãƒ¼: ã‚°ãƒ©ãƒ•ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«build_graph()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        print(f"ğŸ¨ ãƒ„ãƒªãƒ¼å¯è¦–åŒ–ä¸­...")
        
        # ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦å›³ã®ã‚µã‚¤ã‚ºã‚’è‡ªå‹•èª¿æ•´
        if figsize is None:
            total_nodes = self.graph.number_of_nodes()
            if total_nodes > 30:
                figsize = (28, 14)  # å¤§ããªãƒ„ãƒªãƒ¼
            elif total_nodes > 20:
                figsize = (24, 12)  # ä¸­è¦æ¨¡ãƒ„ãƒªãƒ¼
            else:
                figsize = (20, 10)  # å°è¦æ¨¡ãƒ„ãƒªãƒ¼
        
        # å›³ã¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
        
        # éšå±¤ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—
        pos = self._compute_hierarchical_layout()
        
        # ãƒãƒ¼ãƒ‰ã®è‰²è¨­å®š
        node_colors = []
        for node in self.graph.nodes():
            if self.node_types[node] == 'leaf':
                node_colors.append('#90EE90')  # ãƒ©ã‚¤ãƒˆã‚°ãƒªãƒ¼ãƒ³ï¼ˆãƒªãƒ¼ãƒ•ï¼‰
            else:
                depth = self.node_depths[node]
                # æ·±ã•ã«å¿œã˜ã¦è‰²ã‚’å¤‰åŒ–ï¼ˆé’ç³»ã®ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                intensity = 1.0 - (depth * 0.3)
                node_colors.append((0.3, 0.5, intensity))
        
        # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºè¨­å®šï¼ˆã‚ˆã‚Šã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ï¼‰
        node_sizes = []
        for node in self.graph.nodes():
            if self.node_types[node] == 'leaf':
                node_sizes.append(500)  # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã‚’å°‘ã—å¤§ãã
            else:
                depth = self.node_depths[node]
                size = 1000 + (depth * 300)  # å†…éƒ¨ãƒãƒ¼ãƒ‰ã‚‚èª¿æ•´
                node_sizes.append(size)
        
        # ã‚¨ãƒƒã‚¸æç”»
        nx.draw_networkx_edges(
            self.graph, pos, ax=ax,
            edge_color='gray',
            alpha=0.5,
            arrows=True,
            arrowsize=15,
            width=1.5,
            connectionstyle='arc3,rad=0.1'
        )
        
        # ãƒãƒ¼ãƒ‰æç”»
        nx.draw_networkx_nodes(
            self.graph, pos, ax=ax,
            node_color=node_colors,
            node_size=node_sizes,
            alpha=0.9,
            edgecolors='black',
            linewidths=2
        )
        
        # ãƒ©ãƒ™ãƒ«æç”»ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¡¨ç¤ºï¼‰
        labels = nx.get_node_attributes(self.graph, 'label')
        
        # ãƒ©ãƒ™ãƒ«ä½ç½®ã‚’ãƒãƒ¼ãƒ‰ã®å°‘ã—ä¸‹ã«èª¿æ•´
        label_pos = {}
        y_offset = 0.08  # Yè»¸æ–¹å‘ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆãƒãƒ¼ãƒ‰ã®ä¸‹ã«é…ç½®ï¼‰
        for node, (x, y) in pos.items():
            label_pos[node] = (x, y - y_offset)
        
        # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´ï¼ˆãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦ï¼‰
        total_nodes = self.graph.number_of_nodes()
        if total_nodes > 50:
            font_size = 5
        elif total_nodes > 30:
            font_size = 6
        elif total_nodes > 20:
            font_size = 7
        else:
            font_size = 8
        
        nx.draw_networkx_labels(
            self.graph, label_pos, labels, ax=ax,
            font_size=font_size,
            font_weight='bold',
            font_family='MS Gothic',
            bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='gray', alpha=0.85)
        )
        
        # ã‚¿ã‚¤ãƒˆãƒ«è¨­å®š
        tree_name = self.tree_path.stem
        total_nodes = self.graph.number_of_nodes()
        max_depth = max(self.node_depths.values()) + 1
        leaf_count = sum(1 for t in self.node_types.values() if t == 'leaf')
        internal_count = total_nodes - leaf_count
        
        ax.set_title(
            f"RAPTOR Tree: {tree_name}\n"
            f"Total Nodes: {total_nodes} (Leaf: {leaf_count}, Internal: {internal_count}) | "
            f"Max Depth: {max_depth}",
            fontsize=14,
            fontweight='bold',
            pad=20
        )
        
        # å‡¡ä¾‹è¿½åŠ 
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#90EE90', edgecolor='black', label='ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ (Depth 0)'),
            Patch(facecolor=(0.3, 0.5, 0.7), edgecolor='black', label='å†…éƒ¨ãƒãƒ¼ãƒ‰ (Depth 1+)')
        ]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=10)
        
        ax.axis('off')
        plt.tight_layout()
        
        # ä¿å­˜
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… å¯è¦–åŒ–å®Œäº†: {output_path}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_path.stat().st_size / 1024:.1f} KB")
        
        # ã€è¿½åŠ ã€‘è‹±èªç‰ˆã‚‚ä½œæˆ
        self.visualize_tree_en(output_path, figsize, dpi)
    
    def visualize_tree_en(self, ja_output_path: str, figsize: Tuple[int, int] = None, dpi: int = 150):
        """
        ãƒ„ãƒªãƒ¼ã‚’è‹±èªã§å¯è¦–åŒ–ã—ã¦ä¿å­˜ï¼ˆæ—¥æœ¬èªç‰ˆã®è‹±è¨³ï¼‰
        
        Args:
            ja_output_path: æ—¥æœ¬èªç‰ˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            figsize: å›³ã®ã‚µã‚¤ã‚º
            dpi: è§£åƒåº¦
        """
        # è‹±èªç‰ˆã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆ_ENã‚’è¿½åŠ ï¼‰
        ja_path = Path(ja_output_path)
        en_filename = ja_path.stem + '_EN' + ja_path.suffix
        en_output_path = ja_path.parent / en_filename
        
        print(f"ğŸŒ è‹±èªç‰ˆãƒ„ãƒªãƒ¼å¯è¦–åŒ–ä¸­...")
        
        # ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦å›³ã®ã‚µã‚¤ã‚ºã‚’è‡ªå‹•èª¿æ•´
        if figsize is None:
            total_nodes = self.graph.number_of_nodes()
            if total_nodes > 30:
                figsize = (28, 14)
            elif total_nodes > 20:
                figsize = (24, 12)
            else:
                figsize = (20, 10)
        
        # å›³ã¨ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
        
        # éšå±¤ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—ï¼ˆæ—¥æœ¬èªç‰ˆã¨åŒã˜ï¼‰
        pos = self._compute_hierarchical_layout()
        
        # ãƒãƒ¼ãƒ‰ã®è‰²è¨­å®šï¼ˆæ—¥æœ¬èªç‰ˆã¨åŒã˜ï¼‰
        node_colors = []
        for node in self.graph.nodes():
            if self.node_types[node] == 'leaf':
                node_colors.append('#90EE90')
            else:
                depth = self.node_depths[node]
                intensity = 1.0 - (depth * 0.3)
                node_colors.append((0.3, 0.5, intensity))
        
        # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºè¨­å®šï¼ˆæ—¥æœ¬èªç‰ˆã¨åŒã˜ï¼‰
        node_sizes = []
        for node in self.graph.nodes():
            if self.node_types[node] == 'leaf':
                node_sizes.append(500)
            else:
                depth = self.node_depths[node]
                size = 1000 + (depth * 300)
                node_sizes.append(size)
        
        # ã‚¨ãƒƒã‚¸æç”»
        nx.draw_networkx_edges(
            self.graph, pos, ax=ax,
            edge_color='gray',
            alpha=0.5,
            arrows=True,
            arrowsize=15,
            width=1.5,
            connectionstyle='arc3,rad=0.1'
        )
        
        # ãƒãƒ¼ãƒ‰æç”»
        nx.draw_networkx_nodes(
            self.graph, pos, ax=ax,
            node_color=node_colors,
            node_size=node_sizes,
            alpha=0.9,
            edgecolors='black',
            linewidths=2
        )
        
        # ã€é‡è¦ã€‘ãƒ©ãƒ™ãƒ«ã‚’è‹±èªã«ç¿»è¨³
        ja_labels = nx.get_node_attributes(self.graph, 'label')
        en_labels = {}
        for node_id, ja_label in ja_labels.items():
            # ã€Œãƒ»ã€ã§åˆ†å‰²ã—ã¦å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¿»è¨³
            keywords = ja_label.split('ãƒ»')
            en_keywords = [translate_keyword(kw) for kw in keywords]
            en_labels[node_id] = ' Â· '.join(en_keywords)  # è‹±èªã§ã¯ä¸­é»’ã®ä»£ã‚ã‚Šã«ã‚¹ãƒšãƒ¼ã‚¹+Â·+ã‚¹ãƒšãƒ¼ã‚¹
        
        # ãƒ©ãƒ™ãƒ«ä½ç½®ã‚’ãƒãƒ¼ãƒ‰ã®å°‘ã—ä¸‹ã«èª¿æ•´
        label_pos = {}
        y_offset = 0.08
        for node, (x, y) in pos.items():
            label_pos[node] = (x, y - y_offset)
        
        # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´ï¼ˆãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦ï¼‰
        total_nodes = self.graph.number_of_nodes()
        if total_nodes > 50:
            font_size = 5
        elif total_nodes > 30:
            font_size = 6
        elif total_nodes > 20:
            font_size = 7
        else:
            font_size = 8
        
        nx.draw_networkx_labels(
            self.graph, label_pos, en_labels, ax=ax,
            font_size=font_size,
            font_weight='bold',
            font_family='DejaVu Sans',  # è‹±èªãƒ•ã‚©ãƒ³ãƒˆ
            bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='gray', alpha=0.85)
        )
        
        # ã‚¿ã‚¤ãƒˆãƒ«è¨­å®šï¼ˆè‹±èªï¼‰
        tree_name = self.tree_path.stem
        total_nodes = self.graph.number_of_nodes()
        max_depth = max(self.node_depths.values()) + 1
        leaf_count = sum(1 for t in self.node_types.values() if t == 'leaf')
        internal_count = total_nodes - leaf_count
        
        ax.set_title(
            f"RAPTOR Tree: {tree_name}\n"
            f"Total Nodes: {total_nodes} (Leaf: {leaf_count}, Internal: {internal_count}) | "
            f"Max Depth: {max_depth}",
            fontsize=14,
            fontweight='bold',
            pad=20
        )
        
        # å‡¡ä¾‹è¿½åŠ ï¼ˆè‹±èªï¼‰
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#90EE90', edgecolor='black', label='Leaf Node (Depth 0)'),
            Patch(facecolor=(0.3, 0.5, 0.7), edgecolor='black', label='Internal Node (Depth 1+)')
        ]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=10)
        
        ax.axis('off')
        plt.tight_layout()
        
        # ä¿å­˜
        en_output_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(en_output_path, dpi=dpi, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… è‹±èªç‰ˆå¯è¦–åŒ–å®Œäº†: {en_output_path}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {en_output_path.stat().st_size / 1024:.1f} KB")
    
    def _compute_hierarchical_layout(self) -> Dict[str, Tuple[float, float]]:
        """éšå±¤ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¨ˆç®—ï¼ˆãƒ©ãƒ™ãƒ«é‡è¤‡ã‚’é˜²ãï¼‰"""
        pos = {}
        
        # æ·±åº¦ã”ã¨ã«ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        depth_groups = {}
        for node, depth in self.node_depths.items():
            if depth not in depth_groups:
                depth_groups[depth] = []
            depth_groups[depth].append(node)
        
        max_depth = max(self.node_depths.values())
        
        # å„æ·±åº¦ã§å¿…è¦ãªæœ€å°å¹…ã‚’è¨ˆç®—
        # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ãŒå¤šã„å ´åˆã¯å¹…ã‚’åºƒã’ã‚‹
        max_nodes_in_depth = max(len(nodes) for nodes in depth_groups.values())
        
        # ãƒ©ãƒ™ãƒ«ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã®æœ€å°é–“éš”ã‚’è¨­å®š
        # ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã„ã»ã©åºƒã„ç¯„å›²ã«é…ç½®
        if max_nodes_in_depth > 15:
            x_margin = 0.02
            x_range = (0.01, 0.99)
        elif max_nodes_in_depth > 10:
            x_margin = 0.05
            x_range = (0.03, 0.97)
        else:
            x_margin = 0.1
            x_range = (0.05, 0.95)
        
        # å„æ·±åº¦ã®ãƒãƒ¼ãƒ‰ã‚’é…ç½®
        for depth, nodes in depth_groups.items():
            y = max_depth - depth  # ä¸Šã‹ã‚‰ä¸‹ã¸ï¼ˆãƒ«ãƒ¼ãƒˆãŒä¸Šï¼‰
            n = len(nodes)
            
            # Xåº§æ¨™ã‚’å‡ç­‰é…ç½®ï¼ˆåºƒã„ç¯„å›²ã§ï¼‰
            if n == 1:
                x_coords = [0.5]
            else:
                x_coords = np.linspace(x_range[0], x_range[1], n)
            
            for i, node in enumerate(nodes):
                pos[node] = (x_coords[i], y)
        
        return pos
    
    def create_statistics_plot(self, output_path: str):
        """ãƒ„ãƒªãƒ¼çµ±è¨ˆã®ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ"""
        print("ğŸ“Š çµ±è¨ˆãƒ—ãƒ­ãƒƒãƒˆä½œæˆä¸­...")
        
        # æ·±åº¦ã”ã¨ã®ãƒãƒ¼ãƒ‰æ•°é›†è¨ˆ
        depth_counts = {}
        for node, depth in self.node_depths.items():
            depth_counts[depth] = depth_counts.get(depth, 0) + 1
        
        depths = sorted(depth_counts.keys())
        counts = [depth_counts[d] for d in depths]
        
        # ãƒ—ãƒ­ãƒƒãƒˆ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)
        
        # 1. æ·±åº¦åˆ¥ãƒãƒ¼ãƒ‰æ•°
        colors = plt.cm.viridis(np.linspace(0, 1, len(depths)))
        ax1.bar(depths, counts, color=colors, edgecolor='black', linewidth=1.5)
        ax1.set_xlabel('æ·±åº¦ (Depth)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('ãƒãƒ¼ãƒ‰æ•°', fontsize=12, fontweight='bold')
        ax1.set_title('æ·±åº¦åˆ¥ãƒãƒ¼ãƒ‰æ•°', fontsize=14, fontweight='bold')
        ax1.grid(axis='y', alpha=0.3)
        
        # å€¤ã‚’æ£’ã‚°ãƒ©ãƒ•ä¸Šã«è¡¨ç¤º
        for i, (d, c) in enumerate(zip(depths, counts)):
            ax1.text(d, c + max(counts)*0.02, str(c), ha='center', va='bottom', fontweight='bold')
        
        # 2. ãƒ„ãƒªãƒ¼çµ±è¨ˆã‚µãƒãƒªãƒ¼
        total_nodes = sum(counts)
        leaf_nodes = counts[0] if 0 in depth_counts else 0
        internal_nodes = total_nodes - leaf_nodes
        max_depth = max(depths)
        avg_branching = internal_nodes / max(1, max_depth)
        
        stats_text = (
            f"ç·ãƒãƒ¼ãƒ‰æ•°: {total_nodes}\n"
            f"ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰: {leaf_nodes}\n"
            f"å†…éƒ¨ãƒãƒ¼ãƒ‰: {internal_nodes}\n"
            f"æœ€å¤§æ·±åº¦: {max_depth}\n"
            f"å¹³å‡åˆ†å²æ•°: {avg_branching:.2f}\n"
            f"ãƒªãƒ¼ãƒ•æ¯”ç‡: {leaf_nodes/total_nodes*100:.1f}%"
        )
        
        ax2.text(0.5, 0.5, stats_text, 
                 ha='center', va='center',
                 fontsize=14,
                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                 family='MS Gothic')
        ax2.axis('off')
        ax2.set_title('ãƒ„ãƒªãƒ¼çµ±è¨ˆã‚µãƒãƒªãƒ¼', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_path = Path(output_path)
        plt.savefig(output_path, dpi=100, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"âœ… çµ±è¨ˆãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {output_path}")


def visualize_all_trees(
    pkl_dir: str,
    output_dir: str,
    pattern: str = "scaling_test_tree_*.pkl",
    use_morphology: bool = True,
    top_n: int = 2
):
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨RAPTORãƒ„ãƒªãƒ¼ã‚’å¯è¦–åŒ–
    
    Args:
        pkl_dir: pklãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_dir: å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        pattern: ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³
        use_morphology: å½¢æ…‹ç´ è§£æã‚’ä½¿ç”¨ã™ã‚‹ã‹
        top_n: ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«ã«è¡¨ç¤ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰
    """
    pkl_dir = Path(pkl_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # pklãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    pkl_files = list(pkl_dir.glob(pattern))
    
    if not pkl_files:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {pkl_dir} ã« {pattern} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print("=" * 80)
    print("RAPTOR Tree å¯è¦–åŒ–")
    print(f"æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(pkl_files)}")
    print(f"ğŸ§  ãƒ¢ãƒ¼ãƒ‰: {'å½¢æ…‹ç´ è§£æ (MeCab + ç½å®³èªå½™)' if use_morphology else 'TF-IDF + ç½å®³ãƒ‰ãƒ¡ã‚¤ãƒ³èªå½™ãƒ•ã‚£ãƒ«ã‚¿'}")
    print(f"ğŸ·ï¸  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {top_n}å€‹/ãƒãƒ¼ãƒ‰")
    print("=" * 80)
    
    for i, pkl_file in enumerate(sorted(pkl_files), 1):
        print(f"\n[{i}/{len(pkl_files)}] å‡¦ç†ä¸­: {pkl_file.name}")
        print("-" * 80)
        
        try:
            # å¯è¦–åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆå½¢æ…‹ç´ è§£æãƒ¢ãƒ¼ãƒ‰æŒ‡å®šï¼‰
            visualizer = RAPTORTreeVisualizer(pkl_file, use_morphology=use_morphology, top_n=top_n)
            
            # ãƒ„ãƒªãƒ¼èª­ã¿è¾¼ã¿
            visualizer.load_tree()
            
            # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            visualizer.build_graph()
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            base_name = pkl_file.stem
            tree_png = output_dir / f"{base_name}_tree.png"
            stats_png = output_dir / f"{base_name}_stats.png"
            
            # ãƒ„ãƒªãƒ¼å›³ä½œæˆ
            visualizer.visualize_tree(tree_png, figsize=(24, 14), dpi=150)
            
            # çµ±è¨ˆãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
            visualizer.create_statistics_plot(stats_png)
            
            print(f"âœ… [{i}/{len(pkl_files)}] å®Œäº†\n")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {pkl_file.name} ã®å‡¦ç†ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            print(f"   è©³ç´°: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\n" + "=" * 80)
    print("âœ… å…¨ã¦ã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"ğŸ“‚ å‡ºåŠ›å…ˆ: {output_dir}")
    print("=" * 80)


if __name__ == "__main__":
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹è¨­å®š
    pkl_dir = r"C:\Users\yasun\LangChain\learning-langchain\multimodal-raptor-colvbert-blip\data\encoder_comparison_46pdfs\results"
    output_dir = r"C:\Users\yasun\LangChain\learning-langchain\multimodal-raptor-colvbert-blip\data\encoder_comparison_46pdfs\raptor_trees"
    
    # å…¨ãƒ„ãƒªãƒ¼ã‚’å¯è¦–åŒ–ï¼ˆå½¢æ…‹ç´ è§£æãƒ¢ãƒ¼ãƒ‰: MeCabã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰
    # use_morphology=True: MeCabå½¢æ…‹ç´ è§£æã§åè©ãƒ»å›ºæœ‰åè©æŠ½å‡ºã€ç½å®³èªå½™å„ªå…ˆ
    # top_n=2: å„ãƒãƒ¼ãƒ‰ã«2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºï¼ˆå³å¯†ãªéšå±¤çš„é™¤å¤–ï¼‰
    visualize_all_trees(
        pkl_dir=pkl_dir,
        output_dir=output_dir,
        pattern="scaling_test_tree_*.pkl",  # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆ2000, 3000ãªã©ï¼‰
        use_morphology=True,  # å½¢æ…‹ç´ è§£æON: MeCab + UniDicä½“è¨€æ­¢ã‚å½¢å¼
        top_n=2  # 2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºï¼ˆéšå±¤çš„é‡è¤‡ã‚’å³å¯†ã«æ’é™¤ï¼‰
    )
